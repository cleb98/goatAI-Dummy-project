{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# **A step-by-step tutorial for using the COCO dataset in computer vision research.**\n",
    "You can find the comprehensive tutorial in my **[blog post](https://armanasq.github.io/datasets/coco-datset/)**\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive tutorial on the COCO (Common Objects in Context) dataset! \n",
    "\n",
    "The COCO dataset is a widely recognized benchmark in the field of computer vision and serves as a valuable resource for various tasks, including object detection, segmentation, and captioning.\n",
    "\n",
    "In this tutorial, we will delve into the details of the COCO dataset, exploring its purpose, structure, and practical applications. We will cover a range of topics, from installing the required libraries to analyzing the dataset's category distribution and visualizing images with annotations.\n",
    "\n",
    "By the end of this tutorial, you will have a thorough understanding of the COCO dataset and be equipped with the necessary knowledge to leverage its power for your computer vision research and applications.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To follow along with this tutorial, you should have a basic understanding of computer vision concepts and Python programming. Familiarity with libraries such as `numpy`, `matplotlib`, and `seaborn` will be beneficial.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The COCO dataset is a large-scale dataset designed to facilitate research in object recognition, segmentation, and captioning. It contains a diverse collection of images, each annotated with extensive information about the objects present.\n",
    "\n",
    "The key features of the COCO dataset are as follows:\n",
    "\n",
    "- **Object Categories**: The dataset consists of 80 distinct object categories, including common objects like \"person,\" \"car,\" \"dog,\" and more. Each object category is associated with a unique ID.\n",
    "\n",
    "- **Annotations**: For each image in the dataset, precise annotations are provided, including bounding box coordinates, segmentation masks, and category labels. These annotations allow for detailed object localization and understanding.\n",
    "\n",
    "- **Complexity**: The COCO dataset contains images with varying levels of complexity, including multiple objects, occlusions, and diverse backgrounds. This complexity makes it an excellent benchmark for evaluating computer vision algorithms.\n",
    "\n",
    "## Installing Required Libraries\n",
    "\n",
    "Before we dive into working with the COCO dataset, it's essential to ensure that the necessary libraries are installed. Some of the key libraries we will be using are:\n",
    "\n",
    "- `pycocotools`: This library provides a Python API for accessing and manipulating the COCO dataset.\n",
    "\n",
    "- `matplotlib` and `seaborn`: These libraries offer powerful visualization capabilities, allowing us to display images, plots, and annotations.\n",
    "\n",
    "- `numpy`: This fundamental library is used for numerical operations and data manipulation.\n",
    "\n",
    "Ensure that you have these libraries installed by following the installation instructions specific to your system and environment.\n",
    "\n",
    "## Loading COCO Dataset\n",
    "\n",
    "To work with the COCO dataset, you need to obtain the dataset files and organize them on your local machine. The dataset is available for download from the COCO website, and it consists of two main components:\n",
    "\n",
    "- **Images**: The COCO dataset comprises a vast collection of images, grouped into different sets (train, validation, and test). These images form the core of the dataset, providing visual data for various computer vision tasks.\n",
    "\n",
    "- **Annotations**: Alongside the images, the COCO dataset includes detailed annotations for each image. These annotations specify object boundaries, segmentation masks, and category labels. They serve as ground truth data for training and evaluation purposes.\n",
    "\n",
    "Once you have downloaded the dataset, ensure that you have organized the image files and annotations into appropriate directories on your local machine. This organization will make it easier to access and work with the data using the COCO API.\n",
    "\n",
    "## Initializing COCO API\n",
    "\n",
    "To interact with the COCO dataset, we will utilize the COCO API, a Python library that provides convenient access to the dataset's images and annotations.\n",
    "\n",
    "To get started, you need to initialize the COCO API by specifying the paths to the annotation file and the image directory. The annotation file contains the metadata and annotations for the images, while the image directory stores the actual image files.\n",
    "\n",
    "By initializing the COCO API, you can leverage its functionalities to retrieve specific images, annotations, and category information for further analysis and visualization.\n",
    "\n",
    "## Exploring Category Information\n",
    "\n",
    "One crucial aspect of working with the COCO dataset is understanding the object categories it encompasses. Each object in the dataset is assigned a specific category, and it is essential to have an overview of the available categories and their distribution.\n",
    "\n",
    "To explore the category information, we can utilize the COCO API to retrieve category IDs, names, and counts. This information allows us to understand the diversity of object categories in the dataset and their relative frequencies.\n",
    "\n",
    "Analyzing the category distribution is helpful for gaining insights into the dataset's composition and can guide us in formulating strategies for training and evaluating computer vision models.\n",
    "\n",
    "## Loading and Displaying Images\n",
    "\n",
    "A fundamental task when working with the COCO dataset is loading and displaying images. Images provide visual context for the objects and annotations present in the dataset.\n",
    "\n",
    "By utilizing the COCO API, we can load images based on their IDs and retrieve their corresponding annotations. Once the images are loaded, we can display them using libraries such as `matplotlib`. This visual representation helps us get a visual understanding of the dataset and the objects it contains.\n",
    "\n",
    "## Visualizing Category Distribution\n",
    "\n",
    "To gain a comprehensive understanding of the COCO dataset, it is beneficial to visualize the distribution of object categories. This visualization provides insights into the relative frequencies of different object categories, enabling us to identify dominant categories and potentially imbalanced distributions.\n",
    "\n",
    "By leveraging libraries like `matplotlib` and `seaborn`, we can create visualizations such as bar plots and pie charts to represent the category distribution. These visualizations aid in identifying patterns, biases, and potential challenges associated with specific categories.\n",
    "\n",
    "## Filtering Images by Category\n",
    "\n",
    "The COCO dataset is vast, containing images with a diverse range of objects. To focus our analysis on specific object categories of interest, we can filter the images based on those categories.\n",
    "\n",
    "By specifying the desired object categories, we can retrieve the image IDs that contain objects belonging to those categories. This filtering allows us to narrow down the dataset and focus on the specific objects or categories we want to study.\n",
    "\n",
    "## Displaying Filtered Images with Annotations\n",
    "\n",
    "Once we have filtered the images based on specific object categories, we can display the filtered images along with their annotations. This step involves retrieving the annotations corresponding to the filtered images and visualizing them using tools like `matplotlib`.\n",
    "\n",
    "The annotations provide valuable information about object boundaries, segmentation masks, and category labels. By overlaying these annotations on the images, we can gain a comprehensive understanding of the object localization and characteristics within the filtered subset.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The COCO dataset continues to be a valuable resource for the computer vision community, driving advancements in object detection, segmentation, and captioning. By mastering the techniques and concepts covered in this tutorial, you are well-equipped to embark on your own computer vision projects using the COCO dataset.\n",
    "\n",
    "Happy exploring and discovering the rich world of objects in the COCO dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start with the first step, which is installing the pycocotools package.\n",
    "\n",
    "# **Step 1: Installing pycocotools**\n",
    "\n",
    "The pycocotools package is a Python library that provides convenient tools for working with the COCO dataset. It includes APIs for reading and manipulating the annotation files, which are commonly used in computer vision tasks. To install pycocotools, you can follow these steps:\n",
    "\n",
    "1. Open a terminal or command prompt.\n",
    "2. Ensure that you have Python and pip installed on your system.\n",
    "3. Execute the following command:\n",
    "\n",
    "```\n",
    "!pip install pycocotools\n",
    "```\n",
    "\n",
    "This command will download and install the pycocotools package from the Python Package Index (PyPI). Depending on your system configuration, you may need to prefix the command with `sudo` to install the package with administrator privileges.\n",
    "\n",
    "Once the installation is complete, you can proceed to the next steps of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:52:39.310798Z",
     "iopub.status.busy": "2023-07-19T08:52:39.310119Z",
     "iopub.status.idle": "2023-07-19T08:53:15.593823Z",
     "shell.execute_reply": "2023-07-19T08:53:15.592577Z",
     "shell.execute_reply.started": "2023-07-19T08:52:39.310762Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T10:16:37.253204Z",
     "start_time": "2024-12-03T10:16:29.694500Z"
    }
   },
   "source": [
    "!pip install pycocotools"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from pycocotools) (3.9.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from pycocotools) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:53:15.596939Z",
     "iopub.status.busy": "2023-07-19T08:53:15.596251Z",
     "iopub.status.idle": "2023-07-19T08:53:27.117833Z",
     "shell.execute_reply": "2023-07-19T08:53:27.116648Z",
     "shell.execute_reply.started": "2023-07-19T08:53:15.596901Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T10:21:11.239311Z",
     "start_time": "2024-12-03T10:17:22.111016Z"
    }
   },
   "source": [
    "!pip install --upgrade scikit-image scipy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.24.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from scikit-image) (2.1.3)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from scikit-image) (11.0.0)\n",
      "Collecting imageio>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.36.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2024.9.20-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from scikit-image) (24.1)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading scikit_image-0.24.0-cp312-cp312-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.9 MB 322.4 kB/s eta 0:00:39\n",
      "   - -------------------------------------- 0.5/12.9 MB 322.4 kB/s eta 0:00:39\n",
      "   - -------------------------------------- 0.5/12.9 MB 322.4 kB/s eta 0:00:39\n",
      "   -- ------------------------------------- 0.8/12.9 MB 260.1 kB/s eta 0:00:47\n",
      "   -- ------------------------------------- 0.8/12.9 MB 260.1 kB/s eta 0:00:47\n",
      "   --- ------------------------------------ 1.0/12.9 MB 326.9 kB/s eta 0:00:37\n",
      "   --- ------------------------------------ 1.0/12.9 MB 326.9 kB/s eta 0:00:37\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 360.8 kB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 360.8 kB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 322.6 kB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 322.6 kB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 322.6 kB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 330.0 kB/s eta 0:00:34\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 330.0 kB/s eta 0:00:34\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 330.0 kB/s eta 0:00:34\n",
      "   ------ --------------------------------- 2.1/12.9 MB 333.6 kB/s eta 0:00:33\n",
      "   ------ --------------------------------- 2.1/12.9 MB 333.6 kB/s eta 0:00:33\n",
      "   ------ --------------------------------- 2.1/12.9 MB 333.6 kB/s eta 0:00:33\n",
      "   ------ --------------------------------- 2.1/12.9 MB 333.6 kB/s eta 0:00:33\n",
      "   ------- -------------------------------- 2.4/12.9 MB 304.3 kB/s eta 0:00:35\n",
      "   ------- -------------------------------- 2.4/12.9 MB 304.3 kB/s eta 0:00:35\n",
      "   -------- ------------------------------- 2.6/12.9 MB 322.0 kB/s eta 0:00:33\n",
      "   -------- ------------------------------- 2.6/12.9 MB 322.0 kB/s eta 0:00:33\n",
      "   -------- ------------------------------- 2.9/12.9 MB 326.4 kB/s eta 0:00:31\n",
      "   -------- ------------------------------- 2.9/12.9 MB 326.4 kB/s eta 0:00:31\n",
      "   -------- ------------------------------- 2.9/12.9 MB 326.4 kB/s eta 0:00:31\n",
      "   --------- ------------------------------ 3.1/12.9 MB 333.1 kB/s eta 0:00:30\n",
      "   --------- ------------------------------ 3.1/12.9 MB 333.1 kB/s eta 0:00:30\n",
      "   ---------- ----------------------------- 3.4/12.9 MB 350.1 kB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 3.4/12.9 MB 350.1 kB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 354.7 kB/s eta 0:00:27\n",
      "   ------------ --------------------------- 3.9/12.9 MB 365.9 kB/s eta 0:00:25\n",
      "   ------------ --------------------------- 3.9/12.9 MB 365.9 kB/s eta 0:00:25\n",
      "   ------------ --------------------------- 3.9/12.9 MB 365.9 kB/s eta 0:00:25\n",
      "   ------------ --------------------------- 4.2/12.9 MB 364.7 kB/s eta 0:00:25\n",
      "   ------------ --------------------------- 4.2/12.9 MB 364.7 kB/s eta 0:00:25\n",
      "   ------------- -------------------------- 4.5/12.9 MB 374.4 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 4.5/12.9 MB 374.4 kB/s eta 0:00:23\n",
      "   -------------- ------------------------- 4.7/12.9 MB 376.3 kB/s eta 0:00:22\n",
      "   -------------- ------------------------- 4.7/12.9 MB 376.3 kB/s eta 0:00:22\n",
      "   --------------- ------------------------ 5.0/12.9 MB 382.8 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 5.0/12.9 MB 382.8 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 5.0/12.9 MB 382.8 kB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 386.4 kB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 394.8 kB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 5.5/12.9 MB 394.8 kB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 399.9 kB/s eta 0:00:18\n",
      "   ------------------- -------------------- 6.3/12.9 MB 427.3 kB/s eta 0:00:16\n",
      "   --------------------- ------------------ 6.8/12.9 MB 456.9 kB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 7.6/12.9 MB 504.0 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 8.4/12.9 MB 550.9 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 9.7/12.9 MB 628.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 10.0/12.9 MB 643.9 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 10.7/12.9 MB 679.2 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 691.5 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 697.0 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 697.0 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 12.1/12.9 MB 712.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.3/12.9 MB 716.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 730.2 kB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/44.5 MB 2.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 0.8/44.5 MB 3.0 MB/s eta 0:00:15\n",
      "    --------------------------------------- 1.0/44.5 MB 1.9 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.3/44.5 MB 1.2 MB/s eta 0:00:35\n",
      "   - -------------------------------------- 1.8/44.5 MB 1.5 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 2.1/44.5 MB 1.6 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 2.9/44.5 MB 1.8 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 3.7/44.5 MB 1.8 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 5.2/44.5 MB 2.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 6.6/44.5 MB 2.6 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 7.9/44.5 MB 2.9 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 9.2/44.5 MB 3.2 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 10.0/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/44.5 MB 3.0 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.5/44.5 MB 1.7 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 10.7/44.5 MB 1.7 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 11.0/44.5 MB 1.7 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 11.3/44.5 MB 1.7 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 11.8/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 11.8/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 1.6 MB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 12.6/44.5 MB 1.3 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 13.6/44.5 MB 1.4 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 14.4/44.5 MB 1.4 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 15.2/44.5 MB 1.5 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 16.3/44.5 MB 1.6 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 17.0/44.5 MB 1.6 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 18.6/44.5 MB 1.7 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 19.7/44.5 MB 1.8 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 20.4/44.5 MB 1.8 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 21.8/44.5 MB 1.9 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 23.3/44.5 MB 2.0 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 24.1/44.5 MB 2.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 25.7/44.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 27.5/44.5 MB 2.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 28.8/44.5 MB 2.3 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 30.1/44.5 MB 2.3 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 31.5/44.5 MB 2.4 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 33.3/44.5 MB 2.5 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 35.1/44.5 MB 2.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 37.7/44.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 40.9/44.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  44.0/44.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading imageio-2.36.1-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2024.9.20-py3-none-any.whl (228 kB)\n",
      "Installing collected packages: tifffile, scipy, lazy-loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.36.1 lazy-loader-0.4 scikit-image-0.24.0 scipy-1.14.1 tifffile-2024.9.20\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Importing Required Libraries**\n",
    "\n",
    "Before working with the COCO dataset, you need to import certain libraries that will be used for data visualization and processing. In this step, we import the following libraries:\n",
    "\n",
    "- matplotlib: A widely used library for creating visualizations in Python.\n",
    "- matplotlib.pyplot: A sub-module of matplotlib that provides a MATLAB-like interface for creating plots and visualizations.\n",
    "- matplotlib.patches: A submodule of matplotlib that provides various patch classes, such as rectangles, circles, and polygons, which can be used to draw bounding boxes on images.\n",
    "- matplotlib.colors: A submodule of matplotlib that provides functions for working with colors.\n",
    "- seaborn: A library built on top of matplotlib, which provides additional plotting capabilities and enhances the visual aesthetics.\n",
    "- numpy: A fundamental package for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices.\n",
    "\n",
    "Additionally, we import the COCO class from the pycocotools module, which allows us to read and manipulate the COCO dataset's annotation files. To import these libraries and the COCO class, include the following lines of code at the beginning of your script:\n",
    "\n",
    "By importing these libraries, you'll have access to various functions and classes that will be used throughout the tutorial for visualizing and working with the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:53:27.121832Z",
     "iopub.status.busy": "2023-07-19T08:53:27.120642Z",
     "iopub.status.idle": "2023-07-19T08:53:46.778532Z",
     "shell.execute_reply": "2023-07-19T08:53:46.777139Z",
     "shell.execute_reply.started": "2023-07-19T08:53:27.121792Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T10:23:43.373798Z",
     "start_time": "2024-12-03T10:22:54.793147Z"
    }
   },
   "source": [
    "# !pip uninstall -y numpy==1.23.5\n",
    "# !pip install \"numpy>=1.16.5, <1.23.0\"\n",
    "!pip install matplotlib"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cribe\\anaconda3\\envs\\ptbase\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:53:46.782083Z",
     "iopub.status.busy": "2023-07-19T08:53:46.781681Z",
     "iopub.status.idle": "2023-07-19T08:53:46.794553Z",
     "shell.execute_reply": "2023-07-19T08:53:46.793306Z",
     "shell.execute_reply.started": "2023-07-19T08:53:46.782043Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T10:27:15.472207Z",
     "start_time": "2024-12-03T10:26:58.295127Z"
    }
   },
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Setting Up COCO Dataset and Initializing API**\n",
    "\n",
    "In this step, we set up the COCO dataset and initialize the COCO API for working with instance annotations. Follow these instructions to accomplish this:\n",
    "\n",
    "1. Specify the data directory (`dataDir`): Set the variable `dataDir` to the path of your COCO dataset directory. In this example, the COCO dataset is assumed to be located in the directory `/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/`. Modify this path to match the location of your dataset.\n",
    "\n",
    "2. Specify the data type (`dataType`): Set the variable `dataType` to the specific data split you want to work with. In this example, we set it to `'val2014'`, which corresponds to the validation split of the COCO dataset. You can change this value to `'train2014'` or `'test2014'` based on your requirements.\n",
    "\n",
    "3. Specify the annotation file (`annFile`): Use the `annFile` variable to define the path to the COCO dataset's instance annotation file. This file contains the bounding box and category information for each annotated object. In this example, the annotation file path is set as `'annotations/instances_val2014.json'` within the data directory specified earlier. Modify this path accordingly if your dataset follows a different structure.\n",
    "\n",
    "4. Specify the image directory (`imageDir`): Set the `imageDir` variable to the path of the directory containing the dataset images. In this example, the image directory path is set as `'images/val2014/'` within the data directory. Adjust this path if your dataset images are stored in a different location or follow a different naming convention.\n",
    "\n",
    "5. Initialize the COCO API: Create an instance of the COCO class by passing the annotation file path (`annFile`) to the constructor. This initializes the COCO API and allows you to access various methods for interacting with the dataset. In this example, we initialize it as `coco = COCO(annFile)`.\n",
    "\n",
    "By completing these steps, you have set up the COCO dataset and initialized the COCO API, enabling you to access and manipulate the instance annotations and images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:53:52.18588Z",
     "iopub.status.busy": "2023-07-19T08:53:52.185518Z",
     "iopub.status.idle": "2023-07-19T08:54:02.94065Z",
     "shell.execute_reply": "2023-07-19T08:54:02.939662Z",
     "shell.execute_reply.started": "2023-07-19T08:53:52.18585Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T10:51:54.592514Z",
     "start_time": "2024-12-03T10:51:53.235089Z"
    }
   },
   "source": [
    "#use path\n",
    "from pathlib import Path\n",
    "dataDir='/dataset/coco2014/'\n",
    "dataDir = Path(dataDir)\n",
    "dataType='val2014'\n",
    "#crea dataDir\n",
    "dataDir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "annFile='{}annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "imageDir = '{}/images/{}/'.format(dataDir, dataType)\n",
    "\n",
    "\n",
    "# Initialize the COCO api for instance annotations\n",
    "coco=COCO(annFile)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\dataset\\\\coco2014annotations/instances_val2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 14\u001B[0m\n\u001B[0;32m     10\u001B[0m imageDir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/images/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(dataDir, dataType)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Initialize the COCO api for instance annotations\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m coco\u001B[38;5;241m=\u001B[39mCOCO(annFile)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ptbase\\Lib\\site-packages\\pycocotools\\coco.py:81\u001B[0m, in \u001B[0;36mCOCO.__init__\u001B[1;34m(self, annotation_file)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloading annotations into memory...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     80\u001B[0m tic \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 81\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(annotation_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     82\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(dataset)\u001B[38;5;241m==\u001B[39m\u001B[38;5;28mdict\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mannotation file format \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m not supported\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(dataset))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '\\\\dataset\\\\coco2014annotations/instances_val2014.json'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Loading Categories from COCO Dataset**\n",
    "\n",
    "In this step, we load the categories from the COCO dataset based on the provided category IDs. Follow these instructions to load the categories:\n",
    "\n",
    "1. Specify the category ID(s): Set the variable `ids` to the category ID(s) you want to load from the COCO dataset. In this example, `ids` is set to `1`, indicating that we want to load the category with ID 1. You can modify this value to load one or multiple categories based on your requirements.\n",
    "\n",
    "2. Load the categories: Use the `loadCats(ids)` method of the COCO API to load the categories. Pass the `ids` variable as an argument to the method. This method returns a list of category dictionaries containing information such as the category ID, name, and supercategory. In this example, we load the categories and store them in the `cats` variable using the line `cats = coco.loadCats(ids=ids)`.\n",
    "\n",
    "3. Print the categories: Use the `print()` function to display the loaded categories. In this example, we print the `cats` variable, which contains the category information for the provided IDs. This allows you to verify that the correct categories have been loaded.\n",
    "\n",
    "\n",
    "After executing this code, you will see the information about the loaded category/categories printed in the console or output area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.337184Z",
     "iopub.status.busy": "2023-07-17T08:24:54.336799Z",
     "iopub.status.idle": "2023-07-17T08:24:54.342972Z",
     "shell.execute_reply": "2023-07-17T08:24:54.34207Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.337148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load categories for the given ids\n",
    "ids = 1\n",
    "cats = coco.loadCats(ids=ids)\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can print all the categories by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.345188Z",
     "iopub.status.busy": "2023-07-17T08:24:54.34442Z",
     "iopub.status.idle": "2023-07-17T08:24:54.359568Z",
     "shell.execute_reply": "2023-07-17T08:24:54.358629Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.345155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "category_ids = coco.getCatIds()\n",
    "num_categories = len(category_ids)\n",
    "print('number of categories: ',num_categories)\n",
    "for ids in category_ids:\n",
    "    cats = coco.loadCats(ids=ids)\n",
    "    print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5: Loading Images from COCO Dataset**\n",
    "\n",
    "In this step, we load images from the COCO dataset based on the provided image IDs. Follow these instructions to load the images:\n",
    "\n",
    "1. Get the image IDs: Use the `getImgIds()` method of the COCO API to retrieve a list of all image IDs in the dataset. Store this list in the `image_ids` variable.\n",
    "\n",
    "2. Specify the image ID: Set the variable `image_id` to the specific image ID you want to load from the COCO dataset. In this example, we set it as `image_ids[0]`, which corresponds to the first image ID in the list. You can change this line to display a different image by modifying the index value or by selecting a specific image ID from the `image_ids` list.\n",
    "\n",
    "3. Load image information: Use the `loadImgs(image_id)` method of the COCO API to load the image information for the specified image ID. This method returns a list of dictionaries containing details such as the image ID, file name, width, height, and more. In this example, we load the image information and store it in the `image_info` variable using the line `image_info = coco.loadImgs(image_id)`.\n",
    "\n",
    "4. Print the image information: Use the `print()` function to display the loaded image information. In this example, we print the `image_info` variable, which contains the details of the image corresponding to the provided ID.\n",
    "\n",
    "\n",
    "\n",
    "After executing this code, you will see the information about the loaded image printed in the console or output area. This includes details such as the image ID, file name, width, height, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.363401Z",
     "iopub.status.busy": "2023-07-17T08:24:54.362756Z",
     "iopub.status.idle": "2023-07-17T08:24:54.373103Z",
     "shell.execute_reply": "2023-07-17T08:24:54.372147Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.363368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load images for the given ids\n",
    "image_ids = coco.getImgIds()\n",
    "image_id = image_ids[0]  # Change this line to display a different image\n",
    "image_info = coco.loadImgs(image_id)\n",
    "print(image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6: Loading Annotations from COCO Dataset**\n",
    "\n",
    "In this step, we load annotations from the COCO dataset based on the provided image IDs. Follow these instructions to load the annotations:\n",
    "\n",
    "1. Get the annotation IDs: Use the `getAnnIds(imgIds=image_id)` method of the COCO API to retrieve a list of annotation IDs for a specific image. Pass the `image_id` variable as an argument to the method. Store the list of annotation IDs in the `annotation_ids` variable.\n",
    "\n",
    "2. Load annotations: Use the `loadAnns(annotation_ids)` method of the COCO API to load the annotations corresponding to the provided annotation IDs. This method returns a list of annotation dictionaries containing information such as the annotation ID, category ID, bounding box coordinates, and segmentation mask. In this example, we load the annotations and store them in the `annotations` variable using the line `annotations = coco.loadAnns(annotation_ids)`.\n",
    "\n",
    "3. Print the annotations: Use the `print()` function to display the loaded annotations. In this example, we print the `annotations` variable, which contains the details of the annotations for the given image ID.\n",
    "\n",
    "After executing this code, you will see the information about the loaded annotations printed in the console or output area. This includes details such as the annotation ID, category ID, bounding box coordinates, and segmentation mask for each annotation associated with the provided image ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.374978Z",
     "iopub.status.busy": "2023-07-17T08:24:54.374569Z",
     "iopub.status.idle": "2023-07-17T08:24:54.383288Z",
     "shell.execute_reply": "2023-07-17T08:24:54.382256Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.374942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load annotations for the given ids\n",
    "annotation_ids = coco.getAnnIds(imgIds=image_id)\n",
    "annotations = coco.loadAnns(annotation_ids)\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7: Filtering Category IDs based on Given Conditions**\n",
    "\n",
    "In this step, we filter the category IDs from the COCO dataset based on certain conditions. Follow these instructions to filter the category IDs:\n",
    "\n",
    "1. Specify the filter conditions: Set the variable `filterClasses` to a list of category names or classes that you want to filter. In this example, the `filterClasses` list contains the strings `'laptop'`, `'tv'`, and `'cell phone'`. Modify this list to include the desired category names.\n",
    "\n",
    "2. Get the category IDs: Use the `getCatIds(catNms=filterClasses)` method of the COCO API to retrieve the category IDs that satisfy the given filter conditions. Pass the `filterClasses` list as an argument to the method. This method returns a list of category IDs that correspond to the provided category names. In this example, we fetch the category IDs and store them in the `catIds` variable using the line `catIds = coco.getCatIds(catNms=filterClasses)`.\n",
    "\n",
    "3. Print the category IDs: Use the `print()` function to display the filtered category IDs. In this example, we print the `catIds` variable, which contains the category IDs that satisfy the given filter conditions.\n",
    "\n",
    "\n",
    "After executing this code, you will see the filtered category IDs printed in the console or output area. These category IDs correspond to the categories specified in the `filterClasses` list, allowing you to work with specific categories of interest from the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.387308Z",
     "iopub.status.busy": "2023-07-17T08:24:54.386983Z",
     "iopub.status.idle": "2023-07-17T08:24:54.395365Z",
     "shell.execute_reply": "2023-07-17T08:24:54.394417Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.387284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get category ids that satisfy the given filter conditions\n",
    "filterClasses = ['laptop', 'tv', 'cell phone']\n",
    "# Fetch class IDs only corresponding to the filterClasses\n",
    "catIds = coco.getCatIds(catNms=filterClasses)\n",
    "print(catIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 8: Loading Category Information and Filtering Image IDs**\n",
    "\n",
    "In this step, we load category information based on a specific category ID and filter image IDs that satisfy certain conditions. Follow these instructions to perform these tasks:\n",
    "\n",
    "1. Load category information: Set the variable `catID` to the specific category ID for which you want to load information. In this example, `catID` is set to `15`. Modify this value to match the category ID of interest.\n",
    "\n",
    "2. Load category information: Use the `loadCats(ids=catID)` method of the COCO API to load the category information corresponding to the provided category ID. This method returns a list of dictionaries containing details about the category, such as the category ID, name, and supercategory. In this example, we load the category information and print it using the line `print(coco.loadCats(ids=catID))`.\n",
    "\n",
    "3. Filter image IDs: Use the `getImgIds(catIds=[catID])` method of the COCO API to retrieve a list of image IDs that satisfy the given filter conditions. Pass the `catID` variable as an argument to the method within a list. This method returns a list of image IDs corresponding to the specified category ID. In this example, we fetch the image IDs and store the first ID in the variable `imgId` using the line `imgId = coco.getImgIds(catIds=[catID])[0]`.\n",
    "\n",
    "4. Print the image ID: Use the `print()` function to display the filtered image ID. In this example, we print the `imgId` variable, which contains the image ID that satisfies the given filter conditions.\n",
    "\n",
    "Here is an example code snippet:\n",
    "\n",
    "```python\n",
    "# Load category information for the given ID\n",
    "catID = 15\n",
    "print(coco.loadCats(ids=catID))\n",
    "\n",
    "# Get image ID that satisfies the given filter conditions\n",
    "imgId = coco.getImgIds(catIds=[catID])[0]\n",
    "print(imgId)\n",
    "```\n",
    "\n",
    "After executing this code, you will see the category information and the image ID printed in the console or output area. The category information includes details about the category with the provided ID, and the image ID corresponds to an image that belongs to that category in the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.39742Z",
     "iopub.status.busy": "2023-07-17T08:24:54.396495Z",
     "iopub.status.idle": "2023-07-17T08:24:54.407767Z",
     "shell.execute_reply": "2023-07-17T08:24:54.406922Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.397389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "catID = 15\n",
    "print(coco.loadCats(ids=catID))\n",
    "\n",
    "# Get image ids that satisfy the given filter conditions\n",
    "imgId = coco.getImgIds(catIds=[catID])[0]\n",
    "print(imgId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9: Retrieving Annotation IDs for an Image**\n",
    "\n",
    "In this step, we retrieve the annotation IDs for a specific image ID from the COCO dataset. Follow these instructions to accomplish this:\n",
    "\n",
    "1. Get the annotation IDs: Use the `getAnnIds(imgIds=[imgId], iscrowd=None)` method of the COCO API to retrieve the annotation IDs for the given image ID. Pass the `imgId` variable as an argument to the method within a list. Setting `iscrowd` to `None` ensures that both crowd and non-crowd annotations are included. This method returns a list of annotation IDs that correspond to the provided image ID. In this example, we fetch the annotation IDs and store them in the `ann_ids` variable using the line `ann_ids = coco.getAnnIds(imgIds=[imgId], iscrowd=None)`.\n",
    "\n",
    "2. Print the annotation IDs: Use the `print()` function to display the retrieved annotation IDs. In this example, we print the `ann_ids` variable, which contains the annotation IDs associated with the specified image.\n",
    "\n",
    "\n",
    "After executing this code, you will see the annotation IDs printed in the console or output area. These annotation IDs correspond to the annotations of objects present in the specified image ID in the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.409529Z",
     "iopub.status.busy": "2023-07-17T08:24:54.408956Z",
     "iopub.status.idle": "2023-07-17T08:24:54.418558Z",
     "shell.execute_reply": "2023-07-17T08:24:54.417812Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.409498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ann_ids = coco.getAnnIds(imgIds=[imgId], iscrowd=None)\n",
    "print(ann_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 10: Displaying Image with Annotations**\n",
    "\n",
    "In this step, we display an image from the COCO dataset along with its corresponding annotations. Follow these instructions to visualize the image and its annotations:\n",
    "\n",
    "1. Print image information: Use the `print()` function to display information about the image that will be visualized. In this example, we print the file name of the image using the line `print(image_path)`. This helps in identifying the image being displayed.\n",
    "\n",
    "2. Load and display the image: Use the `plt.imread()` function from the matplotlib library to load the image from the specified image directory (`imageDir + image_path`). Store the image in the `image` variable. Then, use the `plt.imshow()` function to display the image.\n",
    "\n",
    "3. Load and display the annotations: Use the `loadAnns()` method of the COCO API to load the annotations corresponding to the provided annotation IDs (`ann_ids`). Store the annotations in the `anns` variable. Then, use the `coco.showAnns()` method to display the annotations on top of the image. Set `draw_bbox=True` to draw bounding boxes around the annotated objects.\n",
    "\n",
    "4. Customize the plot: Use various functions from the `plt` module to customize the plot appearance. Use `plt.axis('off')` to turn off the axis labels, `plt.title()` to set a title for the plot, and `plt.tight_layout()` to optimize the layout of the plot.\n",
    "\n",
    "5. Display the plot: Finally, use `plt.show()` to display the plot with the image and annotations.\n",
    "\n",
    "\n",
    "After executing this code, a plot window will appear showing the image from the COCO dataset with the corresponding annotations displayed on it. The bounding boxes around the annotated objects will be visible, allowing you to visualize the annotations for the specified image ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.420462Z",
     "iopub.status.busy": "2023-07-17T08:24:54.419848Z",
     "iopub.status.idle": "2023-07-17T08:24:54.908846Z",
     "shell.execute_reply": "2023-07-17T08:24:54.90788Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.420431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Annotations for Image ID {imgId}:\")\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "image_path = coco.loadImgs(imgId)[0]['file_name']\n",
    "print(image_path)\n",
    "image = plt.imread(imageDir + image_path)\n",
    "plt.imshow(image)\n",
    "\n",
    "# Display the specified annotations\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Annotations for Image ID: {}'.format(image_id))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 11: Displaying Images with Annotations**\n",
    "\n",
    "In this step, we will load images from the COCO dataset and display them along with their corresponding annotations. This will allow us to visualize the annotated objects in the images. Follow these instructions to accomplish this:\n",
    "\n",
    "1. Retrieve image IDs for specific categories: Use the `getImgIds()` method of the COCO API to retrieve a list of image IDs that contain specific categories. You can specify the category IDs or names using the `catIds` parameter. For example, `img_ids = coco.getImgIds(catIds=[1, 2, 3])` will retrieve image IDs that contain categories with IDs 1, 2, and 3.\n",
    "\n",
    "2. Select an image ID: Choose an image ID from the retrieved list to display. You can change the index value to select a different image. For example, `img_id = img_ids[0]` will select the first image ID from the list.\n",
    "\n",
    "3. Load image information: Use the `loadImgs()` method of the COCO API to load the image information for the selected image ID. This method returns a list of dictionaries containing details about the image, such as the file name, width, height, and URL. For example, `img_info = coco.loadImgs(img_id)[0]` will load the image information for the selected image ID.\n",
    "\n",
    "4. Retrieve annotation IDs: Use the `getAnnIds()` method of the COCO API to retrieve the annotation IDs for the selected image ID. This method returns a list of annotation IDs corresponding to the provided image ID. For example, `ann_ids = coco.getAnnIds(imgIds=[img_id])` will retrieve the annotation IDs for the selected image ID.\n",
    "\n",
    "5. Load annotations: Use the `loadAnns()` method of the COCO API to load the annotations corresponding to the retrieved annotation IDs. This method returns a list of annotation dictionaries containing information such as the category ID, bounding box coordinates, and segmentation mask. For example, `anns = coco.loadAnns(ann_ids)` will load the annotations for the selected image ID.\n",
    "\n",
    "6. Load and display the image: Use a suitable image processing library such as PIL or matplotlib to load and display the image. The image file path can be obtained from the image information dictionary. For example, `image = plt.imread(imageDir + img_info['file_name'])` will load the image using matplotlib.\n",
    "\n",
    "7. Display the annotations on the image: Use the `showAnns()` method of the COCO API to display the annotations on top of the image. This method takes the loaded annotations as input and optionally allows you to draw bounding boxes around the annotated objects. For example, `coco.showAnns(anns, draw_bbox=True)` will display the annotations with bounding boxes.\n",
    "\n",
    "8. Customize the plot: Use various functions from the chosen image processing library to customize the plot appearance. You can add titles, labels, or adjust the axis settings as desired.\n",
    "\n",
    "9. Show the plot: Finally, use the appropriate function from the image processing library to display the plot with the image and annotations. For example, `plt.show()` will display the plot using matplotlib.\n",
    "\n",
    "\n",
    "By following these steps, you will be able to load and display images from the COCO dataset along with their annotations, allowing you to visualize the annotated objects in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:54.910698Z",
     "iopub.status.busy": "2023-07-17T08:24:54.910369Z",
     "iopub.status.idle": "2023-07-17T08:24:55.331722Z",
     "shell.execute_reply": "2023-07-17T08:24:55.330791Z",
     "shell.execute_reply.started": "2023-07-17T08:24:54.910669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Category IDs.\n",
    "    cat_ids = coco.getCatIds()\n",
    "    print(f\"Number of Unique Categories: {len(cat_ids)}\")\n",
    "    print(\"Category IDs:\")\n",
    "    print(cat_ids)  # The IDs are not necessarily consecutive.\n",
    "\n",
    "    # All categories.\n",
    "    cats = coco.loadCats(cat_ids)\n",
    "    cat_names = [cat[\"name\"] for cat in cats]\n",
    "    print(\"Categories Names:\")\n",
    "    print(cat_names)\n",
    "\n",
    "    # Category ID -> Category Name.\n",
    "    query_id = cat_ids[0]\n",
    "    query_annotation = coco.loadCats([query_id])[0]\n",
    "    query_name = query_annotation[\"name\"]\n",
    "    query_supercategory = query_annotation[\"supercategory\"]\n",
    "    print(\"Category ID -> Category Name:\")\n",
    "    print(\n",
    "        f\"Category ID: {query_id}, Category Name: {query_name}, Supercategory: {query_supercategory}\"\n",
    "    )\n",
    "\n",
    "    # Category Name -> Category ID.\n",
    "    query_name = cat_names[2]\n",
    "    query_id = coco.getCatIds(catNms=[query_name])[0]\n",
    "    print(\"Category Name -> ID:\")\n",
    "    print(f\"Category Name: {query_name}, Category ID: {query_id}\")\n",
    "\n",
    "    # Get the ID of all the images containing the object of the category.\n",
    "    img_ids = coco.getImgIds(catIds=[query_id])\n",
    "    print(f\"Number of Images Containing {query_name}: {len(img_ids)}\")\n",
    "\n",
    "    # Pick one image.\n",
    "    img_id = img_ids[2]\n",
    "    img_info = coco.loadImgs([img_id])[0]\n",
    "    img_file_name = img_info[\"file_name\"]\n",
    "    img_url = img_info[\"coco_url\"]\n",
    "    print(\n",
    "        f\"Image ID: {img_id}, File Name: {img_file_name}, Image URL: {img_url}\"\n",
    "    )\n",
    "\n",
    "    # Get all the annotations for the specified image.\n",
    "    ann_ids = coco.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    print(f\"Annotations for Image ID {img_id}:\")\n",
    "    print(anns)\n",
    "\n",
    "    # Use URL to load image.\n",
    "    # im = Image.open(requests.get(img_url, stream=True).raw)\n",
    "    # Load image from dataset\n",
    "    im = plt.imread(imageDir+ coco.loadImgs(img_id)[0]['file_name'])\n",
    "    # Save image and its labeled version.\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.asarray(im))\n",
    "    plt.savefig(f\"{img_id}.jpg\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    # Plot segmentation and bounding box.\n",
    "    coco.showAnns(anns, draw_bbox=True)\n",
    "    plt.savefig(f\"{img_id}_annotated.jpg\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 12: Visualizing Category Distribution in the COCO Dataset**\n",
    "\n",
    "In this step, we will visualize the distribution of categories in the COCO dataset using a horizontal bar plot. This will provide insights into the frequency of different object categories present in the dataset. Follow these instructions to create the plot:\n",
    "\n",
    "1. Load category information: Use the `getCatIds()` method of the COCO API to retrieve the category IDs present in the dataset. Store the category IDs in the `catIDs` variable. Then, use the `loadCats()` method to load the category information corresponding to the category IDs. Store the loaded categories in the `cats` variable.\n",
    "\n",
    "2. Get category names: Extract the category names from the loaded category information. Use a list comprehension to iterate over the `cats` variable and extract the `'name'` key from each category dictionary. Capitalize the category names using the `title()` method and store them in the `category_names` variable.\n",
    "\n",
    "3. Get category counts: Iterate over the category IDs in the `catIDs` variable and use the `getImgIds()` method to retrieve the image IDs associated with each category. Then, calculate the length of each image ID list to obtain the count of images for each category. Store the category counts in the `category_counts` variable.\n",
    "\n",
    "4. Create a color palette: Use the `sns.color_palette()` function from the seaborn library to create a color palette for the plot. Specify the desired color map ('viridis') and the number of colors based on the length of `category_names`. Store the colors in the `colors` variable.\n",
    "\n",
    "5. Create a horizontal bar plot: Create a figure with a specified size using `plt.figure(figsize=(11, 15))`. Use the `sns.barplot()` function from the seaborn library to create the horizontal bar plot. Pass the `category_counts` as the x-values, `category_names` as the y-values, and `colors` as the palette. This will create a bar for each category with its corresponding count.\n",
    "\n",
    "6. Add value labels to the bars: Iterate over the `category_counts` and `category_names` using `enumerate()`. Use the `plt.text()` function to add value labels to each bar. Specify the count as the text, `count + 20` as the x-coordinate for the label (to offset it from the bar), and `i` as the y-coordinate (to align it with the bar).\n",
    "\n",
    "7. Customize the plot: Add labels to the x-axis (`plt.xlabel()`) and y-axis (`plt.ylabel()`), and a title to the plot (`plt.title()`). Adjust the font sizes as desired. Use `plt.tight_layout()` to optimize the layout of the plot.\n",
    "\n",
    "8. Save and show the plot: Use `plt.savefig()` to save the plot as an image file (e.g., 'coco-cats.png') with a specified DPI (e.g., `dpi=300`). Finally, use `plt.show()` to display the plot.\n",
    "\n",
    "After executing this code, a horizontal bar plot will be displayed showing the distribution of categories in the COCO dataset. Each category will be represented by a bar, with the count of images belonging to that category displayed on each bar. The plot provides an overview of the category distribution, helping to understand the composition of the dataset. The plot will also be saved as an image file named 'coco-cats.png' with a DPI of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:55.334048Z",
     "iopub.status.busy": "2023-07-17T08:24:55.333484Z",
     "iopub.status.idle": "2023-07-17T08:24:58.785303Z",
     "shell.execute_reply": "2023-07-17T08:24:58.784319Z",
     "shell.execute_reply.started": "2023-07-17T08:24:55.334015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the categories in a variable\n",
    "catIDs = coco.getCatIds()\n",
    "cats = coco.loadCats(catIDs)\n",
    "\n",
    "# Get category names\n",
    "category_names = [cat['name'].title() for cat in cats]\n",
    "\n",
    "# Get category counts\n",
    "category_counts = [coco.getImgIds(catIds=[cat['id']]) for cat in cats]\n",
    "category_counts = [len(img_ids) for img_ids in category_counts]\n",
    "\n",
    "\n",
    "# Create a color palette for the plot\n",
    "colors = sns.color_palette('viridis', len(category_names))\n",
    "\n",
    "# Create a horizontal bar plot to visualize the category counts\n",
    "plt.figure(figsize=(11, 15))\n",
    "sns.barplot(x=category_counts, y=category_names, palette=colors)\n",
    "\n",
    "# Add value labels to the bars\n",
    "for i, count in enumerate(category_counts):\n",
    "    plt.text(count + 20, i, str(count), va='center')\n",
    "plt.xlabel('Count',fontsize=20)\n",
    "plt.ylabel('Category',fontsize=20)\n",
    "plt.title('Category Distribution in COCO Dataset',fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.savefig('coco-cats.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 13: Visualizing Category Distribution as a Pie Chart**\n",
    "\n",
    "In this step, we will visualize the distribution of categories in the COCO dataset as a pie chart. This type of chart allows for easy comparison of category proportions within the dataset. Follow these instructions to create the pie chart:\n",
    "\n",
    "1. Calculate category percentages: Compute the percentage of each category count out of the total count of all categories. Divide each category count by the total count and multiply by 100 to obtain the category percentages. Store the percentages in the `category_percentages` variable.\n",
    "\n",
    "2. Create the pie chart: Create a figure with a specified size using `plt.figure(figsize=(15, 24.9))`.\n",
    "\n",
    "3. Customize label properties: Define the labels for the pie chart using the category names and corresponding percentages. Add a space after each label for better readability. Customize the label properties, such as font size and background color, using the `label_props` dictionary.\n",
    "\n",
    "4. Add percentage information to labels: Use the `plt.pie()` function to create the pie chart. Pass the category counts as the data. Set `autopct=''` to hide the default percentage labels. Specify `startangle=90` to rotate the pie chart to start from the 90-degree angle (12 o'clock position). Use the `textprops` parameter to apply the label properties defined earlier. Set `pctdistance=0.85` to move the labels away from the center of the pie chart.\n",
    "\n",
    "5. Create the legend: Generate custom legend labels by combining the category labels and percentages. Use the `plt.legend()` function to create the legend. Pass the wedges (created in the previous step), the legend labels, and other parameters such as the title, location, and font size.\n",
    "\n",
    "6. Customize the plot: Adjust the plot aspect ratio using `plt.axis('equal')`. Add a title to the plot using `plt.title()`. Set the font size and adjust the layout of the plot using `plt.tight_layout()`.\n",
    "\n",
    "7. Save and show the plot: Use `plt.savefig()` to save the plot as an image file (e.g., 'coco-dis.png') with a specified DPI (e.g., `dpi=300`). Finally, use `plt.show()` to display the pie chart.\n",
    "\n",
    "\n",
    "After executing this code, a pie chart will be displayed showing the distribution of categories in the COCO dataset. Each category will be represented by a wedge in the pie, with the corresponding count and percentage displayed as labels. The legend will provide a clear overview of the category distribution, and the chart will be saved as an image file named 'coco-dis.png' with a DPI of 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:24:58.787569Z",
     "iopub.status.busy": "2023-07-17T08:24:58.786892Z",
     "iopub.status.idle": "2023-07-17T08:25:04.429015Z",
     "shell.execute_reply": "2023-07-17T08:25:04.428165Z",
     "shell.execute_reply.started": "2023-07-17T08:24:58.787531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate percentage for each category\n",
    "total_count = sum(category_counts)\n",
    "category_percentages = [(count / total_count) * 100 for count in category_counts]\n",
    "\n",
    "\n",
    "# Create a pie chart to visualize the category distribution\n",
    "plt.figure(figsize=(15, 24.9))\n",
    "\n",
    "\n",
    "# Customize labels properties\n",
    "labels = [f\"{name} \" for name, percentage in zip(category_names, category_percentages)]\n",
    "label_props = {\"fontsize\": 25, \n",
    "               \"bbox\": {\"edgecolor\": \"white\", \n",
    "                        \"facecolor\": \"white\", \n",
    "                        \"alpha\": 0.7, \n",
    "                        \"pad\": 0.5}\n",
    "              }\n",
    "\n",
    "# Add percentage information to labels, and set labeldistance to remove labels from the pie\n",
    "wedges, _, autotexts = plt.pie(category_counts, \n",
    "                              autopct='', \n",
    "                              startangle=90, \n",
    "                              textprops=label_props, \n",
    "                              pctdistance=0.85)\n",
    "\n",
    "# Create the legend with percentages\n",
    "legend_labels = [f\"{label}\\n{category_percentages[i]:.1f}%\" for i, label in enumerate(labels)]\n",
    "plt.legend(wedges, legend_labels, title=\"Categories\", loc=\"upper center\", bbox_to_anchor=(0.5, -0.01), \n",
    "           ncol=4, fontsize=12)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.title('Category Distribution in COCO Dataset', fontsize=29)\n",
    "plt.tight_layout()\n",
    "plt.savefig('coco-dis.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 14: Displaying Filtered Images with Annotations**\n",
    "\n",
    "In this step, we will display images from the COCO dataset that contain specific classes, and visualize their annotations. Follow these instructions to accomplish this:\n",
    "\n",
    "1. Define the classes: Create a list called `filterClasses` containing the names of the classes you want to display. Only the images containing these classes will be shown.\n",
    "\n",
    "2. Fetch category IDs: Use the `getCatIds()` method of the COCO API to retrieve the category IDs corresponding to the `filterClasses`. Store the category IDs in the `catIds` variable.\n",
    "\n",
    "3. Get image IDs: Use the `getImgIds()` method of the COCO API to retrieve the image IDs that contain the desired category IDs. Pass the `catIds` variable as an argument to the method. Store the image IDs in the `imgIds` variable.\n",
    "\n",
    "4. Load a random image: Check if there are images in the `imgIds` list. If so, randomly select an image ID using `np.random.randint()`. Load the image information using the `loadImgs()` method and store it in the `image_info` variable.\n",
    "\n",
    "5. Load annotations: Get the annotation IDs for the selected image using the `getAnnIds()` method. Then, load the annotations using the `loadAnns()` method. Store the annotations in the `annotations` variable.\n",
    "\n",
    "6. Get category names and assign colors: Iterate over the annotations and use the `loadCats()` method to retrieve the category names based on the category IDs. Capitalize the category names and store them in the `category_names` variable. Assign colors to each category for visualization purposes.\n",
    "\n",
    "7. Load and display the image: Use `plt.imread()` to load the image using the `image_path` obtained from the image information. Display the image using `plt.imshow()`. Turn off the axis using `plt.axis('off')` and set the title of the plot using `plt.title()`. Save the plot as an image file if desired.\n",
    "\n",
    "8. Display bounding boxes and segmentations: Iterate over the annotations and use the bounding box and segmentation information to draw bounding boxes and segmented regions on the image. Use `patches.Rectangle()` to create bounding box rectangles and `plt.fill()` to display segmentation masks.\n",
    "\n",
    "9. Create a legend: Create a legend to associate the category names with their respective colors. Use `patches.Patch()` to create legend patches and `plt.legend()` to display the legend.\n",
    "\n",
    "10. Display the image with the legend: Show the image with the annotations and legend using `plt.show()`. Adjust the layout if needed.\n",
    "\n",
    "\n",
    "After executing this code, a random image containing the desired classes from the COCO dataset will be displayed with its annotations. The image will include bounding boxes around the objects and segmented regions indicated by different colors. Additionally, a legend will be shown to associate the colors with their respective category names. The plot can be saved as an image file named 'Img.png' (without annotations) and 'annImg.png' (with annotations) if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:25:04.43268Z",
     "iopub.status.busy": "2023-07-17T08:25:04.432043Z",
     "iopub.status.idle": "2023-07-17T08:25:07.733779Z",
     "shell.execute_reply": "2023-07-17T08:25:07.732943Z",
     "shell.execute_reply.started": "2023-07-17T08:25:04.432646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the classes (out of the 80) which you want to see. Others will not be shown.\n",
    "filterClasses = ['laptop', 'tv', 'cell phone']\n",
    "\n",
    "# Fetch class IDs only corresponding to the filterClasses\n",
    "catIds = coco.getCatIds(catNms=filterClasses)\n",
    "\n",
    "# Get all images containing the above Category IDs\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Load a random image from the filtered list\n",
    "if len(imgIds) > 0:\n",
    "    image_id = imgIds[np.random.randint(len(imgIds))]  # Select a random image ID\n",
    "    image_info = coco.loadImgs(image_id)\n",
    "\n",
    "    if image_info is not None and len(image_info) > 0:\n",
    "        image_info = image_info[0]\n",
    "        image_path = imageDir + image_info['file_name']\n",
    "\n",
    "        # Load the annotations for the image\n",
    "        annotation_ids = coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "        # Get category names and assign colors for annotations\n",
    "        category_names = [coco.loadCats(ann['category_id'])[0]['name'].capitalize() for ann in annotations]\n",
    "        category_colors = list(matplotlib.colors.TABLEAU_COLORS.values())\n",
    "\n",
    "        # Load the image and plot it\n",
    "        image = plt.imread(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Annotations for Image ID: {}'.format(image_id))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Img.png',dpi=350)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display bounding boxes and segmented colors for each annotation\n",
    "        for ann, color in zip(annotations, category_colors):\n",
    "            bbox = ann['bbox']\n",
    "            segmentation = ann['segmentation']\n",
    "\n",
    "            # Display bounding box\n",
    "            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1,\n",
    "                                     edgecolor=color, facecolor='none')\n",
    "            plt.gca().add_patch(rect)\n",
    "\n",
    "            # Display segmentation masks with assigned colors\n",
    "            for seg in segmentation:\n",
    "                poly = np.array(seg).reshape((len(seg) // 2, 2))\n",
    "                plt.fill(poly[:, 0], poly[:, 1], color=color, alpha=0.6)\n",
    "\n",
    "        # Create a legend with category names and colors\n",
    "        legend_patches = [patches.Patch(color=color, label=name) for color, name in zip(category_colors, category_names)]\n",
    "        plt.legend(handles=legend_patches, loc=\"lower center\", ncol=4, bbox_to_anchor=(0.5, -0.2), fontsize='small')\n",
    "\n",
    "        # Show the image with legend\n",
    "        plt.title('Annotations for Image ID: {}'.format(image_id))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('annImg.png',dpi=350)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No image information found for the selected image ID.\")\n",
    "else:\n",
    "    print(\"No images found for the desired classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 15: Generating Masks for Object Segmentation**\n",
    "\n",
    "In this step, we will explore the intricacies of generating masks specifically tailored for object segmentation tasks using the COCO dataset. We will delve into the different mask types provided by the dataset, discuss the significance of pixel-level mask annotations, and distinguish between binary and RGB masks. Additionally, we will delve into post-processing techniques, evaluation methods, and leveraging masks for fine-tuning deep learning models.\n",
    "\n",
    "### **Understanding Masks**\n",
    "\n",
    "Masks play a crucial role in object segmentation tasks, providing valuable metadata to highlight specific regions or objects within an image. The COCO dataset offers three main types of masks:\n",
    "\n",
    "1. **Polygon Annotations**: Polygon annotations consist of a series of points connected by straight lines that enclose an object. They provide a set of coordinates representing the boundary of the object. However, using polygon annotations for object segmentation can be challenging due to potential ambiguity in defining accurate boundaries.\n",
    "\n",
    "2. **Instance Segmentation Masks**: Instance segmentation masks are binary or RGB masks that precisely indicate the boundaries separating different objects within an image. These masks provide highly accurate pixel-level segmentation for individual object instances. Instance segmentation masks are widely used in object segmentation tasks due to their accuracy and ability to label specific regions or objects within images.\n",
    "\n",
    "3. **Object Detection Bounding Boxes**: Object detection bounding boxes define rectangular regions that enclose objects within an image. They provide a higher-level representation of objects but lack the pixel-level detail offered by instance segmentation masks. Bounding boxes are less precise when objects overlap or have complex shapes.\n",
    "\n",
    "### **Pixel-Level Mask Annotations**\n",
    "\n",
    "One crucial aspect of masks in the COCO dataset is the provision of pixel-level annotations. These annotations assign a binary value (0 or 1) to each pixel within the mask, enabling precise labeling of specific regions or objects within the image. This level of granularity enhances the accuracy and detail of object segmentation.\n",
    "\n",
    "### **Binary vs RGB Masks**\n",
    "\n",
    "When generating masks for object segmentation tasks, it is important to understand the distinction between binary masks and RGB masks.\n",
    "\n",
    "**Binary masks** are simpler and more efficient, consisting of black and white pixels. White pixels represent the object of interest, while black pixels represent the background. Binary masks are effective for identifying objects within the dataset but may lack nuanced color information.\n",
    "\n",
    "**RGB masks**, on the other hand, provide a more detailed understanding of color information. They employ three color channels (red, green, and blue) with pixel values ranging from 0 to 255. Each pixel value represents a specific object instance within the image, allowing for more precise object separation and distinguishing between closely located or overlapping objects. However, generating RGB masks requires additional processing and can be computationally expensive.\n",
    "\n",
    "### **Generating and Utilizing Masks**\n",
    "\n",
    "To generate masks from the COCO dataset, one can extract the segmented regions or masks using various techniques. This involves accessing the pixel-level information provided by the dataset or employing specialized tools to extract the masks accurately.\n",
    "\n",
    "Furthermore, masks can be utilized in post-processing steps to refine their quality, such as applying morphological operations or smoothing techniques to enhance object boundaries and reduce noise.\n",
    "\n",
    "Evaluating the accuracy of generated masks is paramount. Metrics such as Intersection over Union (IoU) can be used to measure the overlap between predicted masks and ground truth masks, providing insights into the quality of the segmentation.\n",
    "\n",
    "Masks generated from the COCO dataset can also be employed to fine-tune deep learning models, enhancing their performance in object segmentation tasks. By utilizing masks as additional training data, models can learn to better discriminate between object classes and improve overall segmentation accuracy.\n",
    "\n",
    "In conclusion, this step delves into the generation and utilization of masks for object segmentation using the COCO dataset. We explored different mask types, discussed the significance of pixel-level annotations, and highlighted the differences between binary and RGB masks. By comprehending the nuances of mask generation and leveraging them effectively, we can advance the accuracy and precision of object segmentation algorithms.\n",
    "\n",
    "## **Generating Masks for Object Segmentation**\n",
    "\n",
    "In this section, we delve into the intricacies of generating masks specifically tailored for object segmentation using the COCO dataset. We present a systematic approach to extract mask information, generate binary masks, and employ post-processing techniques to enhance mask quality. Furthermore, we discuss evaluation methods to assess the accuracy of generated masks and the potential of leveraging masks for fine-tuning deep learning models.\n",
    "\n",
    "### **Extracting Mask Information**\n",
    "\n",
    "To initiate the mask generation process, meticulous extraction of relevant mask information from the COCO dataset is imperative. Leveraging the COCO API, we load annotations associated with each image ID utilizing the `coco.loadAnns()` function. These annotations provide crucial details including object classes, segmentation polygons/masks, and bounding box coordinates.\n",
    "\n",
    "### **Generating Binary Masks**\n",
    "\n",
    "Binary masks serve as a fundamental representation for object segmentation tasks. Utilizing pixel values of 0 or 1, binary masks effectively denote object absence or presence within the mask. To generate binary masks, we meticulously follow the following steps:\n",
    "\n",
    "1. Retrieve the image ID and corresponding annotations from the COCO dataset.\n",
    "2. Iterate through the annotations, extracting segmentation polygons or masks.\n",
    "3. Transform the segmentation polygons or masks into binary format, assigning a value of 1 to pixels within the object's boundary and 0 to background pixels.\n",
    "4. Instantiate an empty binary mask with dimensions matching that of the image.\n",
    "5. Overlay the binary mask onto the base image, thereby visually accentuating the accurately labeled object.\n",
    "\n",
    "### **Generating RGB Masks**\n",
    "\n",
    "While binary masks provide a simplified representation, RGB masks offer a more nuanced understanding of object segmentation. RGB masks utilize three color channels (red, green, and blue) to differentiate between different classes or objects within an image. Generating RGB masks involves the following steps:\n",
    "\n",
    "1. Retrieve the image ID and corresponding annotations from the COCO dataset.\n",
    "2. Iterate through the annotations, extracting segmentation polygons or masks.\n",
    "3. Assign a unique color to each class or object within the image.\n",
    "4. Instantiate an empty RGB mask with dimensions matching that of the image.\n",
    "5. Overlay the RGB mask onto the base image, showcasing distinct colors for each segmented class or object.\n",
    "\n",
    "RGB masks provide a detailed representation of object boundaries and facilitate more precise separation of objects, enabling finer-grained object segmentation.\n",
    "\n",
    "### **Generating Instance Segmentation Masks**\n",
    "\n",
    "Instance segmentation masks offer a highly accurate representation of object boundaries and facilitate the separation of individual objects within an image. To generate instance segmentation masks, the following steps are followed:\n",
    "\n",
    "1. Retrieve the image ID and corresponding annotations from the COCO dataset.\n",
    "2. Iterate through the annotations, extracting segmentation polygons or masks.\n",
    "3. Generate a binary mask for each object instance, assigning a value of 1 to pixels within the object's boundary and 0 to background pixels.\n",
    "4. Overlay the instance segmentation masks onto the base image, accurately delineating each individual object.\n",
    "\n",
    "Instance segmentation masks provide a pixel-level segmentation of objects, enabling precise labeling and facilitating advanced object-based analyses.\n",
    "\n",
    "### **Generating Object Detection Bounding Boxes**\n",
    "\n",
    "Object detection bounding boxes provide information about the rectangular regions encompassing objects within an image. While they do not offer pixel-level segmentation, they provide a coarse representation of object locations. To generate object detection bounding boxes, the following steps are executed:\n",
    "\n",
    "1. Retrieve the image ID and corresponding annotations from the COCO dataset.\n",
    "2. Extract the bounding box coordinates for each object instance.\n",
    "3. Overlay the bounding boxes onto the base image, visually highlighting the approximate object locations.\n",
    "\n",
    "Object detection bounding boxes serve as a rudimentary means of identifying and localizing objects within an image, albeit with less precision compared to other mask types.\n",
    "\n",
    "In conclusion, this section explored various mask generation techniques for object segmentation. We discussed the generation of binary masks, RGB masks, instance segmentation masks, and object detection bounding boxes. Each mask type offers distinct advantages and can be leveraged based on the specific requirements of the segmentation task at hand. By mastering these techniques, researchers can obtain accurate and detailed masks, enabling advanced object segmentation and analysis.\n",
    "\n",
    "### **Post-Processing Techniques**\n",
    "\n",
    "Post-processing techniques play a pivotal role in refining mask quality and enhancing segmentation accuracy. Several key techniques warrant consideration:\n",
    "\n",
    "1. **Morphological Operations**: Employing morphological operations, such as erosion and dilation, facilitates boundary smoothing, gap filling, and the removal of small, isolated regions within masks.\n",
    "2. **Smoothing Filters**: Leveraging smoothing filters, such as Gaussian blur, mitigates noise and augments the overall quality of the mask.\n",
    "3. **Contour Detection**: Extraction of contours from binary masks provides precise object boundaries, thus enabling further analysis and evaluation.\n",
    "\n",
    "The incorporation of these post-processing techniques can be realized through established libraries such as OpenCV or scikit-image.\n",
    "\n",
    "### **Evaluation of Generated Masks**\n",
    "\n",
    "Accurate evaluation of generated masks is paramount to ensure robust object segmentation. Intersection over Union (IoU), a commonly employed evaluation metric, quantifies the overlap between predicted masks and ground truth masks, thereby furnishing insights into segmentation quality.\n",
    "\n",
    "IoU calculation follows these steps:\n",
    "\n",
    "1. Compute the intersection area between the predicted mask and the ground truth mask.\n",
    "2. Determine the union area by summing the areas of the predicted and ground truth masks, subsequently subtracting the intersection area.\n",
    "3. Divide the intersection area by the union area to derive the IoU score.\n",
    "\n",
    "### **Leveraging Masks for Fine-tuning Models**\n",
    "\n",
    "Masks derived from the COCO dataset provide a valuable resource for fine-tuning deep learning models, ultimately enhancing object segmentation performance. Integration of masks as supplementary training data empowers models to discern object classes more effectively and refine segmentation capabilities.\n",
    "\n",
    "The fine-tuning process entails initializing a pre-trained model with weights and training it on a custom dataset incorporating the generated masks. This iterative training allows the model to adapt to the specific segmentation task, culminating in superior accuracy and refined object separation.\n",
    "\n",
    "By harnessing masks for fine-tuning, models achieve enhanced accuracy, improved object localization, and superior generalization to unseen data.\n",
    "\n",
    "In conclusion, this section elucidates the intricate art of mask generation for object segmentation using the esteemed COCO dataset. We delineate the process of extracting mask information, generating binary masks, employing post-processing techniques, evaluating mask accuracy, and leveraging masks for fine-tuning deep learning models. The profound insights garnered from this comprehensive approach empower researchers to make significant strides in the field of computer vision, transcending traditional boundaries and attaining remarkable results in object segmentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:25:08.792083Z",
     "iopub.status.busy": "2023-07-17T08:25:08.791169Z",
     "iopub.status.idle": "2023-07-17T08:25:08.79864Z",
     "shell.execute_reply": "2023-07-17T08:25:08.797596Z",
     "shell.execute_reply.started": "2023-07-17T08:25:08.792029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extracting Mask Information\n",
    "# Load annotations for a specific image ID\n",
    "# Load images for the given ids\n",
    "image_ids = coco.getImgIds()\n",
    "image_id = image_ids[0] \n",
    "annotations = coco.loadAnns(coco.getAnnIds(imgIds=image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:25:14.53316Z",
     "iopub.status.busy": "2023-07-17T08:25:14.532369Z",
     "iopub.status.idle": "2023-07-17T08:25:15.965625Z",
     "shell.execute_reply": "2023-07-17T08:25:15.963187Z",
     "shell.execute_reply.started": "2023-07-17T08:25:14.533094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve image file path\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "image_dir = os.path.join(dataDir, 'images', 'val2014')\n",
    "image_path = os.path.join(image_dir, image_info['file_name'])\n",
    "\n",
    "# Load the main image\n",
    "main_image = plt.imread(image_path)\n",
    "\n",
    "# Create a new figure for displaying the main image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(main_image)\n",
    "plt.axis('off')\n",
    "plt.title('Main Image')\n",
    "\n",
    "# Save the figures\n",
    "plt.savefig('main_image.png', dpi=300)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Binary Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:25:22.571839Z",
     "iopub.status.busy": "2023-07-17T08:25:22.571467Z",
     "iopub.status.idle": "2023-07-17T08:25:23.556063Z",
     "shell.execute_reply": "2023-07-17T08:25:23.55513Z",
     "shell.execute_reply.started": "2023-07-17T08:25:22.57181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve image dimensions\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "height, width = image_info['height'], image_info['width']\n",
    "\n",
    "# Create an empty binary mask with the same dimensions as the image\n",
    "binary_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "# Iterate through the annotations and draw the binary masks\n",
    "for annotation in annotations:\n",
    "    segmentation = annotation['segmentation']\n",
    "    mask = coco.annToMask(annotation)\n",
    "\n",
    "    # Add the mask to the binary mask\n",
    "    binary_mask += mask\n",
    "\n",
    "# Display the binary mask\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(binary_mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Binary Mask')\n",
    "plt.savefig('binary_mask.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating RGB Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:25:24.471417Z",
     "iopub.status.busy": "2023-07-17T08:25:24.470737Z",
     "iopub.status.idle": "2023-07-17T08:25:25.40205Z",
     "shell.execute_reply": "2023-07-17T08:25:25.401044Z",
     "shell.execute_reply.started": "2023-07-17T08:25:24.471382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve image dimensions\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "height, width = image_info['height'], image_info['width']\n",
    "\n",
    "# Create an empty RGB mask with the same dimensions as the image\n",
    "rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Define a color map for different object classes\n",
    "color_map = {cat['id']: (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))\n",
    "             for cat in coco.loadCats(catIDs)}\n",
    "\n",
    "# Iterate through the annotations and assign unique colors to each class/object\n",
    "for annotation in annotations:\n",
    "    category_id = annotation['category_id']\n",
    "    color = color_map[category_id]\n",
    "\n",
    "    # Draw the mask on the RGB mask\n",
    "    mask = coco.annToMask(annotation)\n",
    "    rgb_mask[mask == 1] = color\n",
    "\n",
    "# Display the RGB mask\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(rgb_mask)\n",
    "plt.axis('off')\n",
    "plt.title('RGB Mask')\n",
    "plt.savefig('rgb_mask.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Instance Segmentation Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:25:26.382465Z",
     "iopub.status.busy": "2023-07-17T08:25:26.381556Z",
     "iopub.status.idle": "2023-07-17T08:25:27.387957Z",
     "shell.execute_reply": "2023-07-17T08:25:27.387024Z",
     "shell.execute_reply.started": "2023-07-17T08:25:26.382418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve image dimensions\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "height, width = image_info['height'], image_info['width']\n",
    "\n",
    "# Create an empty mask with the same dimensions as the image\n",
    "instance_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "# Iterate through the annotations and draw the instance segmentation masks\n",
    "for annotation in annotations:\n",
    "    segmentation = annotation['segmentation']\n",
    "    mask = coco.annToMask(annotation)\n",
    "    category_id = annotation['category_id']\n",
    "\n",
    "    # Assign a unique value to each instance mask\n",
    "    instance_mask[mask == 1] = category_id\n",
    "\n",
    "# Display the instance segmentation mask\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(instance_mask, cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.title('Instance Segmentation Mask')\n",
    "plt.savefig('instance_mask.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Object Detection Bounding Boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:26:17.731663Z",
     "iopub.status.busy": "2023-07-17T08:26:17.731302Z",
     "iopub.status.idle": "2023-07-17T08:26:18.483618Z",
     "shell.execute_reply": "2023-07-17T08:26:18.480767Z",
     "shell.execute_reply.started": "2023-07-17T08:26:17.731632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieve image dimensions\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "height, width = image_info['height'], image_info['width']\n",
    "\n",
    "# Create a new figure with the same dimensions as the image\n",
    "fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n",
    "\n",
    "# Display the original image\n",
    "ax.imshow(main_image)\n",
    "ax.axis('off')\n",
    "ax.set_title('Original Image')\n",
    "\n",
    "# Draw bounding boxes on the original image\n",
    "for annotation in annotations:\n",
    "    bbox = annotation['bbox']\n",
    "    category_id = annotation['category_id']\n",
    "    category_name = coco.loadCats(category_id)[0]['name']\n",
    "\n",
    "    # Convert COCO bounding box format (x, y, width, height) to matplotlib format (xmin, ymin, xmax, ymax)\n",
    "    xmin, ymin, width, height = bbox\n",
    "    xmax = xmin + width\n",
    "    ymax = ymin + height\n",
    "\n",
    "    # Draw the bounding box rectangle\n",
    "    rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Add the category name as a label above the bounding box\n",
    "    ax.text(xmin, ymin - 5, category_name, fontsize=8, color='red', weight='bold')\n",
    "\n",
    "# Save the figure with adjusted dimensions\n",
    "plt.savefig('bounding_boxes.png', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:32:19.424028Z",
     "iopub.status.busy": "2023-07-17T08:32:19.42308Z",
     "iopub.status.idle": "2023-07-17T08:32:22.096995Z",
     "shell.execute_reply": "2023-07-17T08:32:22.096051Z",
     "shell.execute_reply.started": "2023-07-17T08:32:19.423993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import binary_erosion, binary_dilation\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "# Apply erosion to the binary mask\n",
    "eroded_mask = binary_erosion(binary_mask)\n",
    "\n",
    "# Apply dilation to the binary mask\n",
    "dilated_mask = binary_dilation(binary_mask)\n",
    "\n",
    "# Apply Gaussian blur to the binary mask\n",
    "smoothed_mask = gaussian_filter(binary_mask, sigma=.2)\n",
    "\n",
    "# Display the post-processed masks\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "axes[0].imshow(eroded_mask, cmap='gray')\n",
    "axes[0].set_title('Eroded Mask')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(dilated_mask, cmap='gray')\n",
    "axes[1].set_title('Dilated Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(smoothed_mask, cmap='gray')\n",
    "axes[2].set_title('Smoothed Mask')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('post_processed_masks.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Generated Masks\n",
    "\n",
    "Once we have generated masks for object segmentation tasks, it is essential to evaluate their quality and performance. Evaluation metrics provide insights into how well the generated masks align with the ground truth annotations. In this section, we will explore common evaluation metrics used in the context of generated masks.\n",
    "\n",
    "### Intersection over Union (IoU)\n",
    "Intersection over Union (IoU) is a widely used evaluation metric for measuring the similarity between masks. It calculates the ratio of the intersection area to the union area between the predicted mask and the ground truth mask. The IoU score ranges from 0 to 1, with a higher value indicating better alignment between the masks. A score of 1 indicates a perfect match, while a score of 0 suggests no overlap.\n",
    "\n",
    "To calculate the IoU, we can use the numpy library to perform element-wise logical operations on the binary representations of the masks. The intersection and union areas are then computed, and the IoU score is derived by dividing the intersection by the union.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:32:30.055729Z",
     "iopub.status.busy": "2023-07-17T08:32:30.055032Z",
     "iopub.status.idle": "2023-07-17T08:32:30.064719Z",
     "shell.execute_reply": "2023-07-17T08:32:30.063793Z",
     "shell.execute_reply.started": "2023-07-17T08:32:30.055693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ground truth mask\n",
    "gt_mask = binary_mask.astype(bool)  # Example ground truth mask\n",
    "\n",
    "# Predicted mask\n",
    "predicted_mask = smoothed_mask.astype(bool)  # Example predicted mask\n",
    "\n",
    "# Calculate Intersection over Union (IoU)\n",
    "intersection = np.logical_and(gt_mask, predicted_mask)\n",
    "union = np.logical_or(gt_mask, predicted_mask)\n",
    "iou = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "# Print the IoU score\n",
    "print(f\"Intersection over Union (IoU): {iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intersection over Union (IoU) score of 0.6959 indicates the degree of overlap between the predicted mask and the ground truth mask. A higher IoU score suggests a better alignment between the masks, indicating a closer resemblance to the ground truth annotations.\n",
    "\n",
    "In this context, an IoU score of 0.6959 implies a moderate level of overlap and similarity between the predicted and ground truth masks. While it is not a perfect match, it indicates that the predicted mask captures a substantial portion of the objects outlined in the ground truth mask.\n",
    "\n",
    "It is important to consider the specific requirements and objectives of the task when interpreting the IoU score. Depending on the application, a score of 0.6959 might be considered satisfactory or may require further improvement, which can be achieved through fine-tuning the model, adjusting parameters, or exploring alternative segmentation approaches.\n",
    "\n",
    "Keep in mind that the IoU score is just one metric for evaluating the quality of generated masks. It is valuable to consider additional evaluation metrics, visual inspection, and qualitative assessment to obtain a comprehensive understanding of the segmentation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Other Evaluation Metrics\n",
    "In addition to IoU, there are several other evaluation metrics commonly used for mask evaluation. Some of these metrics include:\n",
    "\n",
    "- Pixel Accuracy: Measures the percentage of correctly classified pixels in the predicted mask compared to the ground truth mask.\n",
    "- Precision and Recall: Evaluate the trade-off between true positives, false positives, and false negatives.\n",
    "- F1 Score: Combines precision and recall into a single metric to assess overall performance.\n",
    "- Mean Intersection over Union (mIoU): Computes the average IoU across multiple masks or classes.\n",
    "\n",
    "The choice of evaluation metrics depends on the specific requirements of the task and the nature of the dataset. It is important to select the most appropriate metric(s) based on the objectives and characteristics of the segmentation problem.\n",
    "\n",
    "### Visualization and Qualitative Assessment\n",
    "Apart from numerical evaluation metrics, visual inspection and qualitative assessment of the generated masks are crucial for understanding the performance and identifying potential issues. Visualizing the masks overlaid on the corresponding images can provide insights into the accuracy of the segmentation and highlight areas that may require improvement.\n",
    "\n",
    "Visualization techniques such as color-coded masks, bounding boxes, or contour overlays can help in visually comparing the predicted masks with the ground truth annotations. This visual assessment allows for a more comprehensive evaluation of the generated masks.\n",
    "\n",
    "In summary, evaluating the quality of generated masks is essential to assess the performance of object segmentation algorithms. Utilizing appropriate evaluation metrics and conducting visual assessments can provide valuable insights for refining the models and improving the accuracy of the segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T08:32:33.008227Z",
     "iopub.status.busy": "2023-07-17T08:32:33.007103Z",
     "iopub.status.idle": "2023-07-17T08:32:35.650826Z",
     "shell.execute_reply": "2023-07-17T08:32:35.650013Z",
     "shell.execute_reply.started": "2023-07-17T08:32:33.008183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select an image ID for visualization\n",
    "image_id = image_ids[0]\n",
    "\n",
    "# Load the image\n",
    "image_info = coco.loadImgs(image_id)[0]\n",
    "image_path = os.path.join(imageDir, image_info['file_name'])\n",
    "image = plt.imread(image_path)\n",
    "\n",
    "# Get the ground truth annotations for the image\n",
    "annotation_ids = coco.getAnnIds(imgIds=image_id)\n",
    "annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "# Create a blank image for overlaying the masks\n",
    "overlay = image.copy()\n",
    "\n",
    "# Iterate over the annotations and draw the masks on the overlay image\n",
    "for annotation in annotations:\n",
    "    # Get the segmentation mask\n",
    "    mask = coco.annToMask(annotation)\n",
    "    \n",
    "    # Choose a random color for the mask\n",
    "    color = np.random.randint(0, 256, size=(3,), dtype=np.uint8)\n",
    "    \n",
    "    # Apply the mask to the overlay image\n",
    "    overlay[mask == 1] = color\n",
    "\n",
    "# Create a figure and subplot for visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the original image\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Plot the image with overlay masks\n",
    "ax2.imshow(overlay)\n",
    "ax2.set_title('Masks Overlay')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the visualization as an image file\n",
    "plt.savefig('mask_visualization.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 16: Generating Image and Mask Datasets\n",
    "\n",
    "In this step, we will explore the process of generating image and mask datasets using the COCO dataset. The dataset generation process is a critical step in training deep learning models and performing various computer vision tasks, such as object segmentation and instance segmentation. By generating image and mask datasets, we can provide the necessary inputs and ground truth labels for training our models effectively.\n",
    "\n",
    "## Overview of Dataset Generation\n",
    "\n",
    "The dataset generation process involves creating pairs of images and their corresponding masks. The image dataset contains the original images from the COCO dataset, while the mask dataset contains binary or RGB masks associated with the objects present in the images. These masks represent the regions of interest that need to be segmented or classified by our model.\n",
    "\n",
    "To generate the dataset, we need to follow a series of steps:\n",
    "\n",
    "1. **Data Preprocessing**: In this initial step, we load the COCO dataset using the COCO API and retrieve the necessary annotations and images. The annotations provide valuable information about the objects, their categories, and their corresponding masks. We extract this information to identify the relevant images and their associated annotations for further processing.\n",
    "\n",
    "2. **Image and Mask Pairing**: The next step is to pair each image from the COCO dataset with its corresponding mask. This ensures that each image and mask pair is properly aligned and can be used for training or evaluation purposes. The pairing process requires matching the image IDs with the corresponding mask IDs based on the object instances present in the annotations.\n",
    "\n",
    "3. **Image Augmentation**: To enhance the diversity of our dataset and improve the generalization capabilities of our model, we can apply image augmentation techniques. Image augmentation involves applying random transformations to the images, such as rotation, scaling, flipping, and cropping. These transformations create additional variations of the images and their corresponding masks, thereby increasing the dataset size and introducing more diversity in the training data. It is crucial to ensure that the augmentation is applied consistently to both the images and the masks to maintain their alignment.\n",
    "\n",
    "4. **Splitting the Dataset**: Once the image and mask pairs are generated, it is common practice to split the dataset into training, validation, and test sets. The splitting ratio depends on factors such as the dataset size, the complexity of the task, and the availability of labeled data. The training set is used to train the model, the validation set is used to tune hyperparameters and monitor the model's performance, and the test set is used to evaluate the model's generalization on unseen data. It is essential to ensure that the dataset splitting is done randomly and preserves the distribution of different object classes across the sets.\n",
    "\n",
    "5. **Dataset Storage**: Finally, the generated image and mask datasets need to be stored in a suitable format for easy retrieval during model training and evaluation. Common formats include TFRecord, HDF5, or simply storing the images and masks in separate directories. It is crucial to organize the dataset structure and provide proper indexing or labeling to facilitate efficient data loading and management. Additionally, it is recommended to maintain a separate metadata file that records the mapping between image IDs, mask IDs, and any other relevant information for future reference.\n",
    "\n",
    "## Function: Dataset Generator for COCO\n",
    "\n",
    "To streamline the dataset generation process, we can create a versatile function called `dataset_generator_coco()` specifically designed for generating image and mask datasets from the COCO dataset. This function can be customized based on specific requirements and can take parameters such as `dataDir` (directory path to the COCO dataset), `dataType` (type of dataset: train, val, test), `catIds` (a list of category IDs to filter specific object classes), `imageAugmentation` (a flag indicating whether to apply image augmentation techniques), and `splitRatio` (the ratio for splitting the dataset).\n",
    "\n",
    "The `dataset_generator_coco()` function performs the following steps:\n",
    "\n",
    "1. **Initialize the COCO API and Load Annotations and Images**: In this step, the function initializes the COCO API by providing the path to the COCO dataset. It then loads the necessary annotations and images from the dataset using the COCO API functions.\n",
    "\n",
    "2. **Filter Annotations Based on Object Classes**: Depending on the specific task requirements, we might want to focus on specific object classes. The function filters the annotations based on the provided category IDs (`catIds`) to extract the annotations corresponding to the desired object classes.\n",
    "\n",
    "3. **Pair Images with Masks**: Using the object instance information from the annotations, the function pairs each image with its corresponding mask. By matching the image IDs with the corresponding mask IDs, the function ensures that the image and mask pairs are aligned and can be used together for training or evaluation.\n",
    "\n",
    "4. **Apply Image Augmentation**: To increase the diversity of our dataset and improve the model's ability to generalize, the function can apply image augmentation techniques. The image augmentation process involves applying random transformations to the images, such as rotation, scaling, flipping, and cropping. These transformations should be consistently applied to both the images and masks to maintain their alignment.\n",
    "\n",
    "5. **Split Dataset into Training, Validation, and Test Sets**: After generating the image and mask pairs, it is important to split the dataset into training, validation, and test sets. The splitting ratio can be specified using the `splitRatio` parameter. The function ensures that the splitting is done randomly and preserves the distribution of different object classes across the sets.\n",
    "\n",
    "6. **Store the Generated Datasets**: Finally, the function stores the generated image and mask datasets in a suitable format and directory structure. The datasets can be stored in formats such as TFRecord, HDF5, or as separate directories containing the images and masks. It is crucial to organize the dataset structure properly, provide appropriate indexing or labeling, and maintain a metadata file that records the mapping between image IDs, mask IDs, and any other relevant information for easy retrieval and management.\n",
    "\n",
    "By using the `dataset_generator_coco()` function, researchers and practitioners can streamline the process of generating customized image and mask datasets from the COCO dataset. This function offers flexibility and efficiency, allowing users to tailor the datasets to their specific computer vision tasks and ensuring the availability of properly aligned image and mask pairs.\n",
    "\n",
    "Remember to adapt the function and its parameters to match your specific dataset structure and task requirements. It is also important to maintain proper documentation, including clear instructions on how to use the function and the meaning of each parameter. Additionally, provide detailed information about the dataset structure, file formats, and any preprocessing steps required before using the generated datasets for model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade the tensorflow-io to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:54:16.901737Z",
     "iopub.status.busy": "2023-07-19T08:54:16.900826Z",
     "iopub.status.idle": "2023-07-19T08:54:31.992557Z",
     "shell.execute_reply": "2023-07-19T08:54:31.990832Z",
     "shell.execute_reply.started": "2023-07-19T08:54:16.901691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y tensorflow-io\n",
    "!pip install tensorflow-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:54:31.998434Z",
     "iopub.status.busy": "2023-07-19T08:54:31.998067Z",
     "iopub.status.idle": "2023-07-19T08:55:35.273687Z",
     "shell.execute_reply": "2023-07-19T08:55:35.272046Z",
     "shell.execute_reply.started": "2023-07-19T08:54:31.998383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Load paths for the COCO dataset annotation files\n",
    "ANNOTATION_FILE_TRAIN = '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/annotations/instances_train2014.json'\n",
    "ANNOTATION_FILE_VAL = '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/annotations/instances_val2014.json'\n",
    "\n",
    "# Define the object classes of interest\n",
    "classes = ['person']\n",
    "\n",
    "# Initialize COCO instances for training set and load relevant data\n",
    "coco_train = COCO(ANNOTATION_FILE_TRAIN)\n",
    "catIds_train = coco_train.getCatIds(catNms=classes)\n",
    "imgIds_train = coco_train.getImgIds(catIds=catIds_train)\n",
    "imgDict_train = coco_train.loadImgs(imgIds_train)\n",
    "\n",
    "# Initialize COCO instances for validation set and load relevant data\n",
    "coco_val = COCO(ANNOTATION_FILE_VAL)\n",
    "catIds_val = coco_val.getCatIds(catNms=classes)\n",
    "imgIds_val = coco_val.getImgIds(catIds=catIds_val)\n",
    "imgDict_val = coco_val.loadImgs(imgIds_val)\n",
    "\n",
    "# Print the number of training and validation images and categories\n",
    "print(len(imgIds_train), len(catIds_train))\n",
    "print(len(imgIds_val), len(catIds_val))\n",
    "\n",
    "# Shuffle the training and validation image IDs\n",
    "shuffle(imgIds_train)\n",
    "shuffle(imgIds_val)\n",
    "\n",
    "# Select a subset of validation image IDs\n",
    "imgIds_val = imgIds_val[0:600]\n",
    "imgIds_train = imgIds_train[0:6000]\n",
    "# Generate the list of file names for training and validation person images\n",
    "train_images_person = [\"COCO_train2014_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
    "val_images_person = [\"COCO_val2014_{0:012d}.jpg\".format(ids) for ids in imgIds_val]\n",
    "\n",
    "# Print the number of training and validation person images\n",
    "print(len(train_images_person), len(val_images_person))\n",
    "\n",
    "# Generate the list of file names for training person images\n",
    "train_images_person = [\"COCO_train2014_{0:012d}.jpg\".format(ids) for ids in imgIds_train]\n",
    "\n",
    "# Generate the list of file names for validation person images\n",
    "val_images_person = [\"COCO_val2014_{0:012d}.jpg\".format(ids) for ids in imgIds_val]\n",
    "\n",
    "# Check the number of files in the validation images directory\n",
    "len(os.listdir(\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014\"))\n",
    "\n",
    "# Create a directory to store the generated training masks\n",
    "!mkdir mask_train_2014\n",
    "\n",
    "# Initialize a count variable\n",
    "count = 0 \n",
    "\n",
    "# Generate the masks for training images\n",
    "for ID in imgIds_train:\n",
    "\n",
    "    # Set the file path for the mask\n",
    "    file_path = \"/kaggle/working/mask_train_2014/COCO_train2014_{0:012d}.jpg\".format(ID)\n",
    "\n",
    "    # Retrieve a random image ID from the training set\n",
    "    sampleImgIds = coco_train.getImgIds(imgIds=[ID])\n",
    "    sampleImgDict = coco_train.loadImgs(sampleImgIds[np.random.randint(0, len(sampleImgIds))])[0]\n",
    "\n",
    "    # Retrieve the annotation IDs and annotations for the image\n",
    "    annIds = coco_train.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_train, iscrowd=0)\n",
    "    anns = coco_train.loadAnns(annIds)\n",
    "\n",
    "    # Generate the mask by combining the individual instance masks\n",
    "    mask = coco_train.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask = mask | coco_train.annToMask(anns[i])\n",
    "\n",
    "    # Convert the mask to an image and save it\n",
    "    mask = Image.fromarray(mask * 255, mode=\"L\")\n",
    "    mask.save(file_path)\n",
    "    count = count + 1\n",
    "\n",
    "# Create a directory to store the generated validation masks\n",
    "!mkdir mask_val_2014\n",
    "\n",
    "# Reset the count variable\n",
    "count = 0\n",
    "\n",
    "# Generate the masks for validation images\n",
    "for ID in imgIds_val:\n",
    "\n",
    "    # Set the file path for the mask\n",
    "    file_path = \"/kaggle/working/mask_val_2014/COCO_val2014_{0:012d}.jpg\".format(ID)\n",
    "\n",
    "    # Retrieve a random image ID from the validation set\n",
    "    sampleImgIds = coco_val.getImgIds(imgIds=[ID])\n",
    "    sampleImgDict = coco_val.loadImgs(sampleImgIds[np.random.randint(0, len(sampleImgIds))])[0]\n",
    "\n",
    "    # Retrieve the annotation IDs and annotations for the image\n",
    "    annIds = coco_val.getAnnIds(imgIds=sampleImgDict['id'], catIds=catIds_val, iscrowd=0)\n",
    "    anns = coco_val.loadAnns(annIds)\n",
    "\n",
    "    # Generate the mask by combining the individual instance masks\n",
    "    mask = coco_val.annToMask(anns[0])\n",
    "    for i in range(len(anns)):\n",
    "        mask = mask | coco_val.annToMask(anns[i])\n",
    "\n",
    "    # Convert the mask to an image and save it\n",
    "    mask = Image.fromarray(mask * 255, mode=\"L\")\n",
    "    mask.save(file_path)\n",
    "\n",
    "    count = count + 1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class CustomDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, images_path, masks_path, batch_size):\n",
    "        \"\"\"\n",
    "        CustomDataGenerator class for generating batches of preprocessed images and masks.\n",
    "\n",
    "        Args:\n",
    "            images_path (str): Path to the directory containing the original images.\n",
    "            masks_path (str): Path to the directory containing the corresponding masks.\n",
    "            batch_size (int): Number of samples in each batch.\n",
    "\n",
    "        Attributes:\n",
    "            images_path (str): Path to the directory containing the original images.\n",
    "            masks_path (str): Path to the directory containing the corresponding masks.\n",
    "            batch_size (int): Number of samples in each batch.\n",
    "            image_filenames (list): List of matching filenames between images and masks.\n",
    "            mask_filenames (list): List of matching filenames between masks and images.\n",
    "        \"\"\"\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_filenames = self.get_matching_filenames()\n",
    "        self.mask_filenames = self.get_matching_filenames()\n",
    "\n",
    "    def get_matching_filenames(self):\n",
    "        \"\"\"\n",
    "        Get the list of matching filenames between images and masks.\n",
    "\n",
    "        Returns:\n",
    "            list: List of matching filenames.\n",
    "        \"\"\"\n",
    "        image_files = set([os.path.splitext(filename)[0] for filename in os.listdir(self.images_path)])\n",
    "        mask_files = set([os.path.splitext(filename)[0] for filename in os.listdir(self.masks_path)])\n",
    "        matching_files = list(image_files.intersection(mask_files))\n",
    "        return matching_files\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of batches in the generator.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of batches.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a batch of preprocessed images and masks.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Batch of preprocessed images and masks.\n",
    "        \"\"\"\n",
    "        batch_filenames = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "\n",
    "        for filename in batch_filenames:\n",
    "            image_path = os.path.join(self.images_path, filename + '.jpg')\n",
    "            mask_path = os.path.join(self.masks_path, filename + '.jpg')\n",
    "\n",
    "            image = Image.open(image_path)\n",
    "            mask = Image.open(mask_path)\n",
    "\n",
    "            # Check if image and mask have the same dimensions\n",
    "            if image.size != mask.size:\n",
    "                raise ValueError(f\"Incompatible dimensions for image {image_path} and mask {mask_path}\")\n",
    "\n",
    "            # Resize the images and masks to size 128x128\n",
    "            image = image.resize((128, 128))\n",
    "            mask = mask.resize((128, 128))\n",
    "\n",
    "            # Convert the images and masks to arrays\n",
    "            preprocessed_image = np.array(image)\n",
    "            preprocessed_mask = np.array(mask)\n",
    "\n",
    "            # Check if image has 3 channels and shape of (128, 128, 3)\n",
    "            if len(preprocessed_image.shape) == 3 and preprocessed_image.shape == (128, 128, 3):\n",
    "                # Normalize the pixel values if needed\n",
    "                preprocessed_image = preprocessed_image / 255.0\n",
    "                preprocessed_mask = preprocessed_mask / 255.0\n",
    "\n",
    "                # Append the preprocessed images and masks to the batch\n",
    "                batch_images.append(preprocessed_image)\n",
    "                batch_masks.append(preprocessed_mask)\n",
    "        \n",
    "        # Convert the batch images and masks to numpy arrays and return\n",
    "        return np.array(batch_images), np.array(batch_masks)\n",
    "\n",
    "\n",
    "# Usage\n",
    "images_path = '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014'\n",
    "masks_path = '/kaggle/working/mask_train_2014'\n",
    "batch_size = 8\n",
    "\n",
    "# Create an instance of the CustomDataGenerator\n",
    "train_generator = CustomDataGenerator(images_path, masks_path, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-19T08:48:39.885485Z",
     "iopub.status.busy": "2023-07-19T08:48:39.885084Z",
     "iopub.status.idle": "2023-07-19T08:49:45.770087Z",
     "shell.execute_reply": "2023-07-19T08:49:45.769068Z",
     "shell.execute_reply.started": "2023-07-19T08:48:39.885454Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_image_shapes(generator):\n",
    "    \"\"\"\n",
    "    Print the shapes of preprocessed images generated by the provided generator.\n",
    "\n",
    "    Args:\n",
    "        generator (CustomDataGenerator): Instance of the CustomDataGenerator class.\n",
    "    \"\"\"\n",
    "    for i in range(len(generator)):\n",
    "        # Get a batch of preprocessed images from the generator\n",
    "        batch_images, _ = generator[i]\n",
    "        \n",
    "        # Print the shapes of the preprocessed images\n",
    "        for image in batch_images:\n",
    "            print(f\"Shape of preprocessed image: {image.shape}\")\n",
    "            \n",
    "validate_image_shapes(train_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CustomDataGenerator` class is a custom data generator that generates batches of preprocessed images and masks. It is designed to be used in combination with deep learning models for tasks such as image segmentation.\n",
    "\n",
    "The class has the following methods and attributes:\n",
    "\n",
    "- `__init__(self, images_path, masks_path, batch_size)`: The constructor method that initializes the generator object. It takes the paths to the directories containing the original images (`images_path`), corresponding masks (`masks_path`), and the batch size (`batch_size`). It also initializes the `image_filenames` and `mask_filenames` attributes by calling the `get_matching_filenames()` method.\n",
    "\n",
    "- `get_matching_filenames(self)`: A method that retrieves the matching filenames between the images and masks directories. It returns a list of matching filenames.\n",
    "\n",
    "- `__len__(self)`: A special method that returns the number of batches in the generator. It calculates the number of batches based on the total number of matching filenames and the batch size.\n",
    "\n",
    "- `__getitem__(self, idx)`: A special method that retrieves a batch of preprocessed images and masks. It takes a batch index (`idx`) as input. It retrieves the corresponding filenames for the batch from the `image_filenames` attribute. For each filename, it opens the image and mask, checks if they have the same dimensions, resizes them to 128x128 pixels, converts them to numpy arrays, and performs normalization if needed. The preprocessed images and masks are then appended to the `batch_images` and `batch_masks` lists, respectively. Finally, it converts the lists to numpy arrays and returns the batch of preprocessed images and masks.\n",
    "\n",
    "The `CustomDataGenerator` class provides an efficient way to generate batches of preprocessed images and masks for training deep learning models. To use the generator, you need to instantiate an object by providing the paths to the images directory, masks directory, and the desired batch size. Once instantiated, you can use the object as an iterator to retrieve batches of preprocessed images and masks for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:55:35.284138Z",
     "iopub.status.busy": "2023-07-19T08:55:35.279782Z",
     "iopub.status.idle": "2023-07-19T08:55:35.381895Z",
     "shell.execute_reply": "2023-07-19T08:55:35.380916Z",
     "shell.execute_reply.started": "2023-07-19T08:55:35.284094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print the number of files in the train2014 directory containing original images\n",
    "print(len(os.listdir(\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014\")))\n",
    "\n",
    "# Print the number of files in the mask_train_2014 directory containing generated masks\n",
    "print(len(os.listdir(\"/kaggle/working/mask_train_2014\")))\n",
    "\n",
    "# Print the number of files in the val2014 directory containing original images\n",
    "print(len(os.listdir(\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014\")))\n",
    "\n",
    "# Print the number of files in the mask_val_2014 directory containing generated masks\n",
    "print(len(os.listdir(\"/kaggle/working/mask_val_2014\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the generated masks and images, we can use the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T08:55:35.388469Z",
     "iopub.status.busy": "2023-07-19T08:55:35.386205Z",
     "iopub.status.idle": "2023-07-19T08:55:35.964269Z",
     "shell.execute_reply": "2023-07-19T08:55:35.963344Z",
     "shell.execute_reply.started": "2023-07-19T08:55:35.388435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the list of mask filenames\n",
    "mask_filenames = [filename for filename in os.listdir(masks_path) if filename.endswith('.jpg')]\n",
    "\n",
    "# Assuming the first image in the mask folder is the one to plot\n",
    "first_mask_filename = mask_filenames[0]\n",
    "image_filename = os.path.splitext(first_mask_filename)[0] + '.jpg'\n",
    "\n",
    "# Load and plot the mask image\n",
    "mask_image = Image.open(os.path.join(masks_path, first_mask_filename))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mask_image)\n",
    "plt.title('Mask Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Load and plot the corresponding main image\n",
    "main_image = Image.open(os.path.join(images_path, image_filename))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(main_image)\n",
    "plt.title('Main Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Print the shapes of the images\n",
    "print('Mask Image Shape:', np.array(mask_image).shape)\n",
    "print('Main Image Shape:', np.array(main_image).shape)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {
    "59959ac5-fba7-4687-b396-b90fad3546e8.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 17: Training a U-Net Model for Image Segmentation\n",
    "\n",
    "In this step, we will utilize the dataset generator to train a deep learning model for image segmentation. The model architecture we will use is U-Net, which has proven to be effective for image segmentation tasks.\n",
    "\n",
    "## U-Net Model Overview\n",
    "\n",
    "The U-Net model consists of an encoder and a decoder. The encoder part captures the contextual information and extracts features from the input image, while the decoder part reconstructs the segmented output by performing upsampling and incorporating skip connections.\n",
    "\n",
    "![image.png](attachment:59959ac5-fba7-4687-b396-b90fad3546e8.png)\n",
    "\n",
    "The U-Net architecture has several convolutional layers with batch normalization and max pooling to extract features. It also includes upsampling layers, concatenation of skip connections, and more convolutional layers for the decoder part. The final layer employs a convolutional layer with softmax activation to generate a probability map for each class.\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "1. Import the necessary TensorFlow and Keras modules.\n",
    "\n",
    "2. Specify the input shape of the images. Adjust the dimensions based on the desired input image size.\n",
    "\n",
    "3. Set the number of segmentation classes. This value determines the number of output channels in the final layer.\n",
    "\n",
    "4. Define the model architecture by creating a U-Net model using the provided code. The architecture comprises the encoder and decoder sections with convolutional layers, batch normalization, pooling, upsampling, and skip connections.\n",
    "\n",
    "5. Print a summary of the model, which displays the layers, output shapes, and trainable parameters.\n",
    "\n",
    "6. Set the optimizer for the model. In this case, the Adam optimizer with a learning rate of 1e-4 is used.\n",
    "\n",
    "7. Compile the model with the appropriate loss function and evaluation metric. Here, we use binary cross-entropy loss and accuracy as the metric.\n",
    "\n",
    "8. Train the model by invoking the `fit` method, specifying the training generator and the desired number of epochs.\n",
    "\n",
    "Now let's implement the steps mentioned above and train the U-Net model for image segmentation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:17:50.028319Z",
     "iopub.status.busy": "2023-07-19T09:17:50.02794Z",
     "iopub.status.idle": "2023-07-19T09:17:50.623115Z",
     "shell.execute_reply": "2023-07-19T09:17:50.622289Z",
     "shell.execute_reply.started": "2023-07-19T09:17:50.028286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "def down_block(\n",
    "    input_tensor,\n",
    "    no_filters,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    max_pool_window=(2, 2),\n",
    "    max_pool_stride=(2, 2)\n",
    "):\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_tensor)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    # conv for skip connection\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    pool = MaxPooling2D(pool_size=max_pool_window, strides=max_pool_stride)(conv)\n",
    "\n",
    "    return conv, pool\n",
    "\n",
    "def bottle_neck(\n",
    "    input_tensor,\n",
    "    no_filters,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\"\n",
    "):\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_tensor)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "def up_block(    \n",
    "    input_tensor,\n",
    "    no_filters,\n",
    "    skip_connection, \n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    upsampling_factor = (2,2),\n",
    "    max_pool_window = (2,2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\"):\n",
    "    \n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters = no_filters,\n",
    "        kernel_size= max_pool_window,\n",
    "        strides = strides,\n",
    "        activation = None,\n",
    "        padding = padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(UpSampling2D(size = upsampling_factor)(input_tensor))\n",
    "    \n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv) \n",
    "    \n",
    "    \n",
    "    conv = concatenate( [skip_connection , conv]  , axis = -1)\n",
    "    \n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "\n",
    "    conv = Conv2D(\n",
    "        filters=no_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        strides=strides,\n",
    "        activation=None,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "\n",
    "    conv = BatchNormalization(scale=True)(conv)\n",
    "\n",
    "    conv = Activation(\"relu\")(conv)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "\n",
    "def output_block(input_tensor,\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\"\n",
    "):\n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=2,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        activation=\"relu\",\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(input_tensor)\n",
    "    \n",
    "    \n",
    "    conv = Conv2D(\n",
    "        filters=1,\n",
    "        kernel_size=(1,1),\n",
    "        strides=(1,1),\n",
    "        activation=\"sigmoid\",\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer\n",
    "    )(conv)\n",
    "    \n",
    "    \n",
    "    return conv\n",
    "    \n",
    "\n",
    "def UNet(input_shape = (128,128,3)):\n",
    "    \n",
    "    filter_size = [64,128,256,512,1024]\n",
    "    \n",
    "    inputs = Input(shape = input_shape)\n",
    "    \n",
    "    d1 , p1 = down_block(input_tensor= inputs,\n",
    "                         no_filters=filter_size[0],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    d2 , p2 = down_block(input_tensor= p1,\n",
    "                         no_filters=filter_size[1],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    d3 , p3 = down_block(input_tensor= p2,\n",
    "                         no_filters=filter_size[2],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    d4 , p4 = down_block(input_tensor= p3,\n",
    "                         no_filters=filter_size[3],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\",\n",
    "                         max_pool_window=(2,2),\n",
    "                         max_pool_stride=(2,2))\n",
    "    \n",
    "    \n",
    "    b = bottle_neck(input_tensor= p4,\n",
    "                         no_filters=filter_size[4],\n",
    "                         kernel_size = (3,3),\n",
    "                         strides=(1,1),\n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    u4 = up_block(input_tensor = b,\n",
    "                  no_filters = filter_size[3],\n",
    "                  skip_connection = d4,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    u3 = up_block(input_tensor = u4,\n",
    "                  no_filters = filter_size[2],\n",
    "                  skip_connection = d3,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    u2 = up_block(input_tensor = u3,\n",
    "                  no_filters = filter_size[1],\n",
    "                  skip_connection = d2,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    u1 = up_block(input_tensor = u2,\n",
    "                  no_filters = filter_size[0],\n",
    "                  skip_connection = d1,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=(1, 1),\n",
    "                  upsampling_factor = (2,2),\n",
    "                  max_pool_window = (2,2),\n",
    "                  padding=\"same\",\n",
    "                  kernel_initializer=\"he_normal\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = output_block(input_tensor=u1 , \n",
    "                         padding = \"same\",\n",
    "                         kernel_initializer= \"he_normal\")\n",
    "    \n",
    "    model = keras.models.Model(inputs = inputs , outputs = output)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = UNet(input_shape = (128,128,3))\n",
    "model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can plot the model by using the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-19T08:55:39.549549Z",
     "iopub.status.busy": "2023-07-19T08:55:39.549177Z",
     "iopub.status.idle": "2023-07-19T08:55:40.290899Z",
     "shell.execute_reply": "2023-07-19T08:55:40.289335Z",
     "shell.execute_reply.started": "2023-07-19T08:55:39.549514Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Visualize the model\n",
    "plot_model(model, to_file='unet_model.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:17:55.78784Z",
     "iopub.status.busy": "2023-07-19T09:17:55.78747Z",
     "iopub.status.idle": "2023-07-19T09:17:55.990504Z",
     "shell.execute_reply": "2023-07-19T09:17:55.989454Z",
     "shell.execute_reply.started": "2023-07-19T09:17:55.78781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images_path = '/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014'\n",
    "masks_path = '/kaggle/working/mask_val_2014'\n",
    "batch_size = 8\n",
    "\n",
    "val_generator = CustomDataGenerator(images_path, masks_path, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-07-19T09:11:09.176028Z",
     "iopub.status.busy": "2023-07-19T09:11:09.175294Z",
     "iopub.status.idle": "2023-07-19T09:11:14.995196Z",
     "shell.execute_reply": "2023-07-19T09:11:14.994126Z",
     "shell.execute_reply.started": "2023-07-19T09:11:09.175994Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_preprocessed_image_shapes(model, generator):\n",
    "    \"\"\"\n",
    "    Print the shapes of preprocessed images generated by the provided model and generator.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained model.\n",
    "        generator (CustomDataGenerator): Instance of the CustomDataGenerator class.\n",
    "    \"\"\"\n",
    "    for i in range(len(generator)):\n",
    "        # Get a batch of preprocessed images from the generator\n",
    "        batch_images, batch_mask = generator[i]\n",
    "\n",
    "        # Pass the batch of images through the model to obtain predictions\n",
    "        # predictions = model.predict(batch_images)\n",
    "\n",
    "        # Print the shapes of the preprocessed images\n",
    "        for image in batch_images:\n",
    "            print(f\"Shape of preprocessed image: {image.shape}\")\n",
    "            \n",
    "            \n",
    "# Print the shapes of preprocessed images\n",
    "print_preprocessed_image_shapes(model, val_generator)\n",
    "\n",
    "\n",
    "\n",
    "#import os\n",
    "#os.environ['TF_DISABLE_MODEL_OPTIMIZATIONS'] = '1'\n",
    "#!pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:28:51.301812Z",
     "iopub.status.busy": "2023-07-19T09:28:51.301418Z",
     "iopub.status.idle": "2023-07-19T09:28:51.308149Z",
     "shell.execute_reply": "2023-07-19T09:28:51.307201Z",
     "shell.execute_reply.started": "2023-07-19T09:28:51.301775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T09:18:00.119815Z",
     "iopub.status.busy": "2023-07-19T09:18:00.119444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fit the model with the training generator\n",
    "\n",
    "train_steps =  len(os.listdir( \"/kaggle/working/mask_train_2014/\"))/batch_size\n",
    "model.fit(train_generator,validation_data = val_generator, steps_per_epoch = train_steps , epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 18: Predicting and Visualizing Segmentation Masks\n",
    "\n",
    "In this step, we will use the trained U-Net model to predict segmentation masks for a sample batch of validation data. We will also visualize the sample image, ground truth mask, and predicted mask for one random example in the batch.\n",
    "\n",
    "## Prediction and Visualization Overview\n",
    "\n",
    "To predict segmentation masks, we will follow these steps:\n",
    "\n",
    "1. Create a validation data generator using the `CustomDataGenerator` class. This generator will provide batches of validation images and masks.\n",
    "\n",
    "2. Get a sample batch from the validation data generator to use for prediction. This can be done by indexing the generator, similar to accessing elements in an array.\n",
    "\n",
    "3. Generate predictions on the sample batch using the trained U-Net model.\n",
    "\n",
    "4. Apply a threshold to the predictions if necessary. This step converts the predicted probabilities to binary values (0 or 1) based on a specified threshold.\n",
    "\n",
    "5. Select a random index from the batch to visualize one example.\n",
    "\n",
    "6. Plot the sample image, ground truth mask, and predicted mask using matplotlib.\n",
    "\n",
    "7. Adjust the layout and spacing of the subplots for better visualization.\n",
    "\n",
    "8. Show the figure to display the sample image, ground truth mask, and predicted mask.\n",
    "\n",
    "Now let's implement the steps mentioned above to predict and visualize the segmentation masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-18T21:42:53.060002Z",
     "iopub.status.busy": "2023-07-18T21:42:53.059089Z",
     "iopub.status.idle": "2023-07-18T21:42:54.987958Z",
     "shell.execute_reply": "2023-07-18T21:42:54.986954Z",
     "shell.execute_reply.started": "2023-07-18T21:42:53.059955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get a sample batch from the validation data generator\n",
    "sample_images, sample_masks = val_generator[0]\n",
    "\n",
    "# Generate predictions on the sample batch\n",
    "predictions = model.predict(sample_images)\n",
    "\n",
    "# Threshold the predictions (if needed)\n",
    "threshold = 0.5  # Adjust the threshold as per your requirement\n",
    "thresholded_predictions = (predictions > threshold).astype(np.uint8)\n",
    "\n",
    "# Select a random index from the batch\n",
    "idx = np.random.randint(0, sample_images.shape[0])\n",
    "\n",
    "# Plot the sample image, ground truth mask, and predicted mask\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot sample image\n",
    "axes[0].imshow(sample_images[idx])\n",
    "axes[0].set_title('Sample Image')\n",
    "\n",
    "# Plot ground truth mask\n",
    "axes[1].imshow(sample_masks[idx])\n",
    "axes[1].set_title('Ground Truth Mask')\n",
    "\n",
    "# Plot predicted mask\n",
    "axes[2].imshow(thresholded_predictions[idx])\n",
    "axes[2].set_title('Predicted Mask')\n",
    "\n",
    "# Set common title for the figure\n",
    "fig.suptitle('Sample Image, Ground Truth Mask, and Predicted Mask')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1573501,
     "sourceId": 2598787,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
